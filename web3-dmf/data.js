const DMP_DATA = [
  {
    "id": "DGA.01",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "Dual-speed Governance",
    "weight": 3,
    "objective": "Enable strategic alignment of business and IT through dual-speed planning and delivery models.",
    "assessmentQuestions": "1. Does the organization implement both agile and traditional planning models in parallel?",
    "testingProcedures": "1. Review strategic planning documents\n2. Interview stakeholders on governance alignment.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4 Strategic Planning\n3. NIST SP 800-53 PM family",
    "supportingTech": "1. Enterprise Architecture Tools (e.g., LeanIX)\n2. Strategy Execution Platforms (e.g., Cascade, WorkBoard)",
    "evidenceArtifacts": "1. Strategic Plans\n2. Solution Roadmaps\n3. Architecture Artifacts",
    "relatedProcesses": "Portfolio Management, IT Governance, Change Management",
    "stakeholders": "CIO, CTO, Enterprise Architects, Program Managers",
    "riskImplication": "Misalignment between IT and business goals; delayed modernization efforts",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.01.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "Agile Strategy Sprints",
    "weight": 7,
    "objective": "Implement Agile Strategy Sprints to mature Bi-modal Strategic Planning and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Does leadership conduct recurring agile strategy sprints?\n2. Are sprint outcomes fed back into portfolio and architectural planning",
    "testingProcedures": "1. Review sprint artifacts (strategy backlogs, sprint reviews).\n2. Interview PMO/Architects regarding cadence and alignment.",
    "complianceFrameworks": "1. COBIT APO02\n2. NIST SP 800-53 PM-9\n3. ISO 27001 A.5 Governance",
    "supportingTech": "1. Jira Align\n2. Miro strategic boards\n3. Cascade OKR platform",
    "evidenceArtifacts": "1. Strategy backlog\n2. Sprint review notes\n3. Quarterly strategy adjustments",
    "relatedProcesses": "1. Portfolio Management\n2. Architecture Governance\n3. IT Governance",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Agile Strategy Sprints causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.01.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "Quarterly Business Reviews",
    "weight": 7,
    "objective": "Implement Quarterly Business Reviews to mature Bi-modal Strategic Planning and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Quarterly Business Reviews defined, owned, and implemented consistently across the organization?\n2. Is Quarterly Business Reviews measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Quarterly Business Reviews.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Business performance dashboards (Power BI, Tableau)\n2. Portfolio/OKR platforms (WorkBoard, Ally.io)\n3. PPM tools (Jira Align, Planview)",
    "evidenceArtifacts": "1. Quarterly Business Reviews artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Quarterly Business Reviews causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.01.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "Architectural Runway Planning",
    "weight": 7,
    "objective": "Implement Architectural Runway Planning to mature Bi-modal Strategic Planning and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Architectural Runway Planning defined, owned, and implemented consistently across the organization?\n2. Is Architectural Runway Planning measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Architectural Runway Planning.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Architectural Runway Planning artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Architectural Runway Planning causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.01.4",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "Objectives & Key Results (OKR) Reporting",
    "weight": 8,
    "objective": "Implement Objectives & Key Results (OKR) Reporting to mature Bi-modal Strategic Planning and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Objectives & Key Results (OKR) Reporting defined, owned, and implemented consistently across the organization?\n2. Is Objectives & Key Results (OKR) Reporting measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Objectives & Key Results (OKR) Reporting.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. OKR/strategy execution (WorkBoard, Ally.io)\n2. Portfolio/OKR platforms (WorkBoard, Ally.io)\n3. PPM tools (Jira Align, Planview)",
    "evidenceArtifacts": "1. Objectives & Key Results (OKR) Reporting artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Objectives & Key Results (OKR) Reporting causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.01.5",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "Balance Scorecard",
    "weight": 7,
    "objective": "Implement Balance Scorecard to mature Bi-modal Strategic Planning and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Balance Scorecard defined, owned, and implemented consistently across the organization?\n2. Is Balance Scorecard measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Balance Scorecard.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Balance Scorecard artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Balance Scorecard causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.01.6",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Bi-modal Strategic Planning",
    "subDescription": "Balances traditional strategic planning with fast, agile delivery models to support rapid digital change.",
    "practice": "PMBOK Strategic Alignment",
    "weight": 7,
    "objective": "Implement PMBOK Strategic Alignment to mature Bi-modal Strategic Planning and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is PMBOK Strategic Alignment defined, owned, and implemented consistently across the organization?\n2. Is PMBOK Strategic Alignment measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for PMBOK Strategic Alignment.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. PMBOK Strategic Alignment artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature PMBOK Strategic Alignment causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Portfolio Management",
    "subDescription": "Optimizes technology investments through structured prioritization and value assessment.",
    "practice": "Portfolio Intake",
    "weight": 7,
    "objective": "Implement Portfolio Intake to mature Digital Portfolio Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Portfolio Intake defined, owned, and implemented consistently across the organization?\n2. Is Portfolio Intake measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Portfolio Intake.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Portfolio Intake artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Portfolio Intake causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Portfolio Management",
    "subDescription": "Optimizes technology investments through structured prioritization and value assessment.",
    "practice": "Prioritization Scoring",
    "weight": 7,
    "objective": "Implement Prioritization Scoring to mature Digital Portfolio Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Prioritization Scoring defined, owned, and implemented consistently across the organization?\n2. Is Prioritization Scoring measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Prioritization Scoring.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Prioritization Scoring artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Prioritization Scoring causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Portfolio Management",
    "subDescription": "Optimizes technology investments through structured prioritization and value assessment.",
    "practice": "Lean Funding",
    "weight": 7,
    "objective": "Implement Lean Funding to mature Digital Portfolio Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Lean Funding defined, owned, and implemented consistently across the organization?\n2. Is Lean Funding measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Lean Funding.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Portfolio/OKR platforms (WorkBoard, Ally.io)\n3. PPM tools (Jira Align, Planview)",
    "evidenceArtifacts": "1. Lean Funding artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Lean Funding causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Portfolio Management",
    "subDescription": "Optimizes technology investments through structured prioritization and value assessment.",
    "practice": "Dependency Mapping",
    "weight": 7,
    "objective": "Implement Dependency Mapping to mature Digital Portfolio Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Dependency Mapping defined, owned, and implemented consistently across the organization?\n2. Is Dependency Mapping measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Dependency Mapping.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Dependency Mapping artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Dependency Mapping causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.4",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Portfolio Management",
    "subDescription": "Optimizes technology investments through structured prioritization and value assessment.",
    "practice": "Portfolio ROI",
    "weight": 7,
    "objective": "Implement Portfolio ROI to mature Digital Portfolio Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Portfolio ROI defined, owned, and implemented consistently across the organization?\n2. Is Portfolio ROI measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Portfolio ROI.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Portfolio ROI artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Portfolio ROI causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.5",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Portfolio Management",
    "subDescription": "Optimizes technology investments through structured prioritization and value assessment.",
    "practice": "Time-to-value Analytics",
    "weight": 7,
    "objective": "Implement Time-to-value Analytics to mature Digital Portfolio Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Time-to-value Analytics defined, owned, and implemented consistently across the organization?\n2. Is Time-to-value Analytics measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Time-to-value Analytics.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Time-to-value Analytics artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Time-to-value Analytics causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Spend Management",
    "subDescription": "Ensures cost transparency and optimization across cloud and IT assets.",
    "practice": "FinOps",
    "weight": 7,
    "objective": "Implement FinOps to mature Digital Spend Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is FinOps defined, owned, and implemented consistently across the organization?\n2. Is FinOps measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for FinOps.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. FinOps artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature FinOps causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Spend Management",
    "subDescription": "Ensures cost transparency and optimization across cloud and IT assets.",
    "practice": "Cost Anomaly Detection",
    "weight": 8,
    "objective": "Implement Cost Anomaly Detection to mature Digital Spend Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Cost Anomaly Detection defined, owned, and implemented consistently across the organization?\n2. Is Cost Anomaly Detection measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cost Anomaly Detection.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Cost Anomaly Detection artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Cost Anomaly Detection causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Spend Management",
    "subDescription": "Ensures cost transparency and optimization across cloud and IT assets.",
    "practice": "Cloud Consumption Guardrails",
    "weight": 7,
    "objective": "Implement Cloud Consumption Guardrails to mature Digital Spend Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Cloud Consumption Guardrails defined, owned, and implemented consistently across the organization?\n2. Is Cloud Consumption Guardrails measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cloud Consumption Guardrails.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Cloud Consumption Guardrails artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Cloud Consumption Guardrails causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Spend Management",
    "subDescription": "Ensures cost transparency and optimization across cloud and IT assets.",
    "practice": "Unit Cost Benchmarking",
    "weight": 7,
    "objective": "Implement Unit Cost Benchmarking to mature Digital Spend Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Unit Cost Benchmarking defined, owned, and implemented consistently across the organization?\n2. Is Unit Cost Benchmarking measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Unit Cost Benchmarking.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Unit Cost Benchmarking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Unit Cost Benchmarking causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.4",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Spend Management",
    "subDescription": "Ensures cost transparency and optimization across cloud and IT assets.",
    "practice": "Budget Adherence",
    "weight": 7,
    "objective": "Implement Budget Adherence to mature Digital Spend Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Budget Adherence defined, owned, and implemented consistently across the organization?\n2. Is Budget Adherence measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Budget Adherence.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Budget Adherence artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Budget Adherence causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.5",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Data-Driven Decision Making",
    "subDescription": "Enables leadership to make decisions using real-time, validated enterprise data.",
    "practice": "Data Literacy Programs",
    "weight": 7,
    "objective": "Implement Data Literacy Programs to mature Data-Driven Decision Making and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Data Literacy Programs defined, owned, and implemented consistently across the organization?\n2. Is Data Literacy Programs measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data Literacy Programs.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Data Literacy Programs artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Data Literacy Programs causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Data-Driven Decision Making",
    "subDescription": "Enables leadership to make decisions using real-time, validated enterprise data.",
    "practice": "Automated Dashboards",
    "weight": 7,
    "objective": "Implement Automated Dashboards to mature Data-Driven Decision Making and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Automated Dashboards defined, owned, and implemented consistently across the organization?\n2. Is Automated Dashboards measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated Dashboards.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Automated Dashboards artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Automated Dashboards causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Data-Driven Decision Making",
    "subDescription": "Enables leadership to make decisions using real-time, validated enterprise data.",
    "practice": "Predictive Analytics Adoption",
    "weight": 7,
    "objective": "Implement Predictive Analytics Adoption to mature Data-Driven Decision Making and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Predictive Analytics Adoption defined, owned, and implemented consistently across the organization?\n2. Is Predictive Analytics Adoption measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Predictive Analytics Adoption.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Predictive Analytics Adoption artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Predictive Analytics Adoption causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.02.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Data-Driven Decision Making",
    "subDescription": "Enables leadership to make decisions using real-time, validated enterprise data.",
    "practice": "Quantitiative Decision Making",
    "weight": 7,
    "objective": "Implement Quantitiative Decision Making to mature Data-Driven Decision Making and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Quantitiative Decision Making defined, owned, and implemented consistently across the organization?\n2. Is Quantitiative Decision Making measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Quantitiative Decision Making.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Quantitiative Decision Making artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Quantitiative Decision Making causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Adaptive Business Risk Management",
    "subDescription": "Establishes real-time, automated, and scenario-based enterprise risk governance.",
    "practice": "Continuous Enterprise Risk Monitoring",
    "weight": 8,
    "objective": "Implement Continuous Enterprise Risk Monitoring to mature Adaptive Business Risk Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Continuous Enterprise Risk Monitoring defined, owned, and implemented consistently across the organization?\n2. Is Continuous Enterprise Risk Monitoring measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Continuous Enterprise Risk Monitoring.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Continuous Enterprise Risk Monitoring artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Continuous Enterprise Risk Monitoring causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Adaptive Business Risk Management",
    "subDescription": "Establishes real-time, automated, and scenario-based enterprise risk governance.",
    "practice": "Risk Quantification",
    "weight": 8,
    "objective": "Implement Risk Quantification to mature Adaptive Business Risk Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Risk Quantification defined, owned, and implemented consistently across the organization?\n2. Is Risk Quantification measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Risk Quantification.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Risk Quantification artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Risk Quantification causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Adaptive Business Risk Management",
    "subDescription": "Establishes real-time, automated, and scenario-based enterprise risk governance.",
    "practice": "Periodic Measure of Appetite & Tolerances",
    "weight": 8,
    "objective": "Implement Periodic Measure of Appetite & Tolerances to mature Adaptive Business Risk Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Periodic Measure of Appetite & Tolerances defined, owned, and implemented consistently across the organization?\n2. Is Periodic Measure of Appetite & Tolerances measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Periodic Measure of Appetite & Tolerances.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Portfolio/OKR platforms (WorkBoard, Ally.io)\n3. PPM tools (Jira Align, Planview)",
    "evidenceArtifacts": "1. Periodic Measure of Appetite & Tolerances artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Periodic Measure of Appetite & Tolerances causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.03.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Adaptive Business Risk Management",
    "subDescription": "Establishes real-time, automated, and scenario-based enterprise risk governance.",
    "practice": "Risk trending, susceptibility and velocity metrics",
    "weight": 8,
    "objective": "Implement Risk trending, susceptibility and velocity metrics to mature Adaptive Business Risk Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Risk trending, susceptibility and velocity metrics defined, owned, and implemented consistently across the organization?\n2. Is Risk trending, susceptibility and velocity metrics measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Risk trending, susceptibility and velocity metrics.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Risk trending, susceptibility and velocity metrics artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Risk trending, susceptibility and velocity metrics causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.04",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Supply-Chain Management",
    "subDescription": "Modernizes procurement, logistics, and vendor operations through digital automation.",
    "practice": "Real-time inventory tracking",
    "weight": 8,
    "objective": "Implement Real-time inventory tracking to mature Digital Supply-Chain Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Real-time inventory tracking defined, owned, and implemented consistently across the organization?\n2. Is Real-time inventory tracking measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Real-time inventory tracking.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Portfolio/OKR platforms (WorkBoard, Ally.io)\n3. PPM tools (Jira Align, Planview)",
    "evidenceArtifacts": "1. Real-time inventory tracking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Real-time inventory tracking causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.04.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Supply-Chain Management",
    "subDescription": "Modernizes procurement, logistics, and vendor operations through digital automation.",
    "practice": "Supplier Portals",
    "weight": 8,
    "objective": "Implement Supplier Portals to mature Digital Supply-Chain Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Supplier Portals defined, owned, and implemented consistently across the organization?\n2. Is Supplier Portals measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Supplier Portals.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Supplier Portals artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Supplier Portals causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.04.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Supply-Chain Management",
    "subDescription": "Modernizes procurement, logistics, and vendor operations through digital automation.",
    "practice": "Logistics Automation",
    "weight": 8,
    "objective": "Implement Logistics Automation to mature Digital Supply-Chain Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Logistics Automation defined, owned, and implemented consistently across the organization?\n2. Is Logistics Automation measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Logistics Automation.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. Portfolio/OKR platforms (WorkBoard, Ally.io)\n2. PPM tools (Jira Align, Planview)\n3. EA repositories (LeanIX, Alfabet)",
    "evidenceArtifacts": "1. Logistics Automation artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Logistics Automation causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.04.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital Leadership",
    "coreDescription": "Successful modernization and transformation requires strong leadership capabilities",
    "subDimension": "Digital Supply-Chain Management",
    "subDescription": "Modernizes procurement, logistics, and vendor operations through digital automation.",
    "practice": "Measuring Supplier conformity, SLA adherence",
    "weight": 8,
    "objective": "Implement Measuring Supplier conformity, SLA adherence to mature Digital Supply-Chain Management and improve Digital Leadership outcomes.",
    "assessmentQuestions": "1. Is Measuring Supplier conformity, SLA adherence defined, owned, and implemented consistently across the organization?\n2. Is Measuring Supplier conformity, SLA adherence measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Measuring Supplier conformity, SLA adherence.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. ISO/IEC 38500\n4. TOGAF (EA)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Portfolio/OKR platforms (WorkBoard, Ally.io)\n3. PPM tools (Jira Align, Planview)",
    "evidenceArtifacts": "1. Measuring Supplier conformity, SLA adherence artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Strategy & Portfolio Management, Enterprise Architecture, IT Governance, Change Enablement",
    "stakeholders": "CEO, CIO, CTO, CDO, Business Unit Leaders, Enterprise Architects, PMO",
    "riskImplication": "Immature Measuring Supplier conformity, SLA adherence causes misalignment, stalled modernization, and inconsistent prioritization of investments.",
    "transformationInitiative": "Establish Dual-Speed Governance Model & PMO Transformation",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.05",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Innovation Feedback Loops",
    "subDescription": "Creates structured pathways for rapid experimentation and continuous innovation.",
    "practice": "Design Thinking",
    "weight": 3,
    "objective": "Evaluate and mitigate risks originating from third-party vendors and partners.",
    "assessmentQuestions": "Does the organization perform risk assessments for all critical vendors?",
    "testingProcedures": "1. Review Vendor inventories\n2. Review Risk tiers \n3. Review Onboarding assessments",
    "complianceFrameworks": "1. NIST SP 800-161\n2. ISO 27036\n3. CIS Controls v8 (Control 15)",
    "supportingTech": "1. Third-party risk platforms (e.g., OneTrust, ProcessUnity)\n2. Contract risk scoring tools",
    "evidenceArtifacts": "1. Design Thinking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "1. Vendor inventory\n2. Vendor security and risk due diligence reports\n3. Vendor SLAs",
    "stakeholders": "1. Vendor Management\n2. Procurement\n3. Risk Governance",
    "riskImplication": "Hidden vulnerabilities from third-party dependencies",
    "transformationInitiative": "Launch vendor risk scoring and onboarding automation program",
    "industryOverlay": "1. Healthcare\n2. Financial Services\n3. Tech"
  },
  {
    "id": "DGA.05.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Innovation Feedback Loops",
    "subDescription": "Creates structured pathways for rapid experimentation and continuous innovation.",
    "practice": "Rapid Prototyping",
    "weight": 7,
    "objective": "Implement Rapid Prototyping to mature Innovation Feedback Loops and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Rapid Prototyping defined, owned, and implemented consistently across the organization?\n2. Is Rapid Prototyping measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Rapid Prototyping.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Rapid Prototyping artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Rapid Prototyping increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Launch vendor risk scoring and onboarding automation program",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.05.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Innovation Feedback Loops",
    "subDescription": "Creates structured pathways for rapid experimentation and continuous innovation.",
    "practice": "Experimentation Sandboxes",
    "weight": 7,
    "objective": "Implement Experimentation Sandboxes to mature Innovation Feedback Loops and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Experimentation Sandboxes defined, owned, and implemented consistently across the organization?\n2. Is Experimentation Sandboxes measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Experimentation Sandboxes.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Experimentation Sandboxes artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Experimentation Sandboxes increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Launch vendor risk scoring and onboarding automation program",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.05.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Innovation Feedback Loops",
    "subDescription": "Creates structured pathways for rapid experimentation and continuous innovation.",
    "practice": "Measuring Innovation cycle-time, experiment success rates.",
    "weight": 7,
    "objective": "Implement Measuring Innovation cycle-time, experiment success rates. to mature Innovation Feedback Loops and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Measuring Innovation cycle-time, experiment success rates. defined, owned, and implemented consistently across the organization?\n2. Is Measuring Innovation cycle-time, experiment success rates. measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Measuring Innovation cycle-time, experiment success rates..\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Change mgmt tooling (Prosci toolkit)\n3. Intranet/Comms (SharePoint, Viva Engage)",
    "evidenceArtifacts": "1. Measuring Innovation cycle-time, experiment success rates. artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Measuring Innovation cycle-time, experiment success rates. increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Launch vendor risk scoring and onboarding automation program",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.06",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Digital Adoption & Engagement",
    "subDescription": "Drives enterprise-wide adoption of modern digital tools and platforms.",
    "practice": "Persona-based Onboarding",
    "weight": 7,
    "objective": "Implement Persona-based Onboarding to mature Digital Adoption & Engagement and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Persona-based Onboarding defined, owned, and implemented consistently across the organization?\n2. Is Persona-based Onboarding measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Persona-based Onboarding.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Persona-based Onboarding artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Persona-based Onboarding increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.06.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Digital Adoption & Engagement",
    "subDescription": "Drives enterprise-wide adoption of modern digital tools and platforms.",
    "practice": "Change Impact Modeling",
    "weight": 7,
    "objective": "Implement Change Impact Modeling to mature Digital Adoption & Engagement and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Change Impact Modeling defined, owned, and implemented consistently across the organization?\n2. Is Change Impact Modeling measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Change Impact Modeling.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Change Impact Modeling artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Change Impact Modeling increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.06.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Digital Adoption & Engagement",
    "subDescription": "Drives enterprise-wide adoption of modern digital tools and platforms.",
    "practice": "Measuring time to proficiency, usage heatmaps",
    "weight": 7,
    "objective": "Implement Measuring time to proficiency, usage heatmaps to mature Digital Adoption & Engagement and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Measuring time to proficiency, usage heatmaps defined, owned, and implemented consistently across the organization?\n2. Is Measuring time to proficiency, usage heatmaps measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Measuring time to proficiency, usage heatmaps.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Change mgmt tooling (Prosci toolkit)\n3. Intranet/Comms (SharePoint, Viva Engage)",
    "evidenceArtifacts": "1. Measuring time to proficiency, usage heatmaps artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Measuring time to proficiency, usage heatmaps increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.07",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Digital Evangelism",
    "subDescription": "Builds a culture where employees advocate for new tools, digital processes, and innovation",
    "practice": "CoE Frameworks",
    "weight": 7,
    "objective": "Implement CoE Frameworks to mature Digital Evangelism and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is CoE Frameworks defined, owned, and implemented consistently across the organization?\n2. Is CoE Frameworks measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for CoE Frameworks.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. CoE Frameworks artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature CoE Frameworks increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.07.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Digital Evangelism",
    "subDescription": "Builds a culture where employees advocate for new tools, digital processes, and innovation",
    "practice": "Capability Academies",
    "weight": 7,
    "objective": "Implement Capability Academies to mature Digital Evangelism and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Capability Academies defined, owned, and implemented consistently across the organization?\n2. Is Capability Academies measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Capability Academies.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Capability Academies artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Capability Academies increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.08",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Remote Worker Enablement",
    "subDescription": "Ensures secure, seamless productivity for hybrid and remote teams",
    "practice": "Zero-trust Remote Access",
    "weight": 7,
    "objective": "Implement Zero-trust Remote Access to mature Remote Worker Enablement and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Zero-trust Remote Access defined, owned, and implemented consistently across the organization?\n2. Is Zero-trust Remote Access measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Zero-trust Remote Access.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Zero-trust Remote Access artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Zero-trust Remote Access increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.08.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Remote Worker Enablement",
    "subDescription": "Ensures secure, seamless productivity for hybrid and remote teams",
    "practice": "Collaboration Optimization",
    "weight": 7,
    "objective": "Implement Collaboration Optimization to mature Remote Worker Enablement and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Collaboration Optimization defined, owned, and implemented consistently across the organization?\n2. Is Collaboration Optimization measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Collaboration Optimization.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Collaboration Optimization artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Collaboration Optimization increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.09",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Agile Change Management",
    "subDescription": "Develops an adaptive change capability aligned with agile development.",
    "practice": "Continuous Change Pipelines",
    "weight": 7,
    "objective": "Implement Continuous Change Pipelines to mature Agile Change Management and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Continuous Change Pipelines defined, owned, and implemented consistently across the organization?\n2. Is Continuous Change Pipelines measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Continuous Change Pipelines.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Continuous Change Pipelines artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Continuous Change Pipelines increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.09.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Agile Change Management",
    "subDescription": "Develops an adaptive change capability aligned with agile development.",
    "practice": "Stakeholder Mapping",
    "weight": 7,
    "objective": "Implement Stakeholder Mapping to mature Agile Change Management and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Stakeholder Mapping defined, owned, and implemented consistently across the organization?\n2. Is Stakeholder Mapping measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Stakeholder Mapping.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Stakeholder Mapping artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Stakeholder Mapping increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.09.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Agile Change Management",
    "subDescription": "Develops an adaptive change capability aligned with agile development.",
    "practice": "Change Backlogs",
    "weight": 7,
    "objective": "Implement Change Backlogs to mature Agile Change Management and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Change Backlogs defined, owned, and implemented consistently across the organization?\n2. Is Change Backlogs measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Change Backlogs.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. Change mgmt tooling (Prosci toolkit)\n2. Intranet/Comms (SharePoint, Viva Engage)\n3. Survey/VoC tools (Qualtrics, CultureAmp)",
    "evidenceArtifacts": "1. Change Backlogs artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Change Backlogs increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.09.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Organization & Culture",
    "coreDescription": "Digital transformation requires an organization that's all in and readily adopts new ways of doing things",
    "subDimension": "Agile Change Management",
    "subDescription": "Develops an adaptive change capability aligned with agile development.",
    "practice": "Measuring Change success rate, change lead-time",
    "weight": 7,
    "objective": "Implement Measuring Change success rate, change lead-time to mature Agile Change Management and improve Organization & Culture outcomes.",
    "assessmentQuestions": "1. Is Measuring Change success rate, change lead-time defined, owned, and implemented consistently across the organization?\n2. Is Measuring Change success rate, change lead-time measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Measuring Change success rate, change lead-time.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. COBIT 2019\n2. ITIL 4\n3. NIST CSF 2.0 (Govern)\n4. ISO 27001:2022 (People & governance)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Change mgmt tooling (Prosci toolkit)\n3. Intranet/Comms (SharePoint, Viva Engage)",
    "evidenceArtifacts": "1. Measuring Change success rate, change lead-time artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports",
    "relatedProcesses": "Org Change Management, Communications, Training & Enablement, Workforce Planning",
    "stakeholders": "CHRO, CIO, CISO, BU Leaders, Change Management Lead, Communications",
    "riskImplication": "Immature Measuring Change success rate, change lead-time increases execution risk and slows delivery of modernization outcomes.",
    "transformationInitiative": "Modernize operating model and change management capability",
    "industryOverlay": "1. All Industries"
  },
  {
    "id": "DGA.10",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Digital Security Program Optimization",
    "subDescription": "Establishes an adaptive, risk-driven cybersecurity program aligned to modernization.",
    "practice": "Continuous Control Testing",
    "weight": 2,
    "objective": "Ensure policies are up-to-date, accessible, and aligned with compliance requirements.",
    "assessmentQuestions": "Is there a centralized system for maintaining and reviewing policies?",
    "testingProcedures": "1. Check central repositories\n2. Review Audit logs\n3. Review Version history of critical policies.",
    "complianceFrameworks": "1. ISO 27001: A.5\n2. NIST CSF (ID.GV-1)",
    "supportingTech": "1. GRC platforms (e.g., ServiceNow, ZenGRC)\n2. Document Control Systems",
    "evidenceArtifacts": "1. Continuous Control Testing artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "1. Review Policy register\n2. Review Policy attestation records",
    "stakeholders": "1. Compliance\n2. Internal Audit\n3. Legal",
    "riskImplication": "Noncompliance and policy drift across teams",
    "transformationInitiative": "Digitize and automate policy review and approval workflows",
    "industryOverlay": "All Industries"
  },
  {
    "id": "DGA.10.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Digital Security Program Optimization",
    "subDescription": "Establishes an adaptive, risk-driven cybersecurity program aligned to modernization.",
    "practice": "SOC Automation",
    "weight": 10,
    "objective": "Implement SOC Automation to mature Digital Security Program Optimization and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is SOC Automation defined, owned, and implemented consistently across the organization?\n2. Is SOC Automation measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for SOC Automation.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. SIEM/SOAR (Sentinel, Splunk, XSOAR)\n2. GRC platform (ServiceNow GRC, OneTrust, Archer)\n3. Policy mgmt/workflow (ServiceNow, Confluence)",
    "evidenceArtifacts": "1. SOC Automation artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate SOC Automation leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate policy review and approval workflows",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.10.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Digital Security Program Optimization",
    "subDescription": "Establishes an adaptive, risk-driven cybersecurity program aligned to modernization.",
    "practice": "Maturity Assessments",
    "weight": 9,
    "objective": "Implement Maturity Assessments to mature Digital Security Program Optimization and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Maturity Assessments defined, owned, and implemented consistently across the organization?\n2. Is Maturity Assessments measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Maturity Assessments.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Maturity Assessments artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Maturity Assessments leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate policy review and approval workflows",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.10.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Digital Security Program Optimization",
    "subDescription": "Establishes an adaptive, risk-driven cybersecurity program aligned to modernization.",
    "practice": "Security Dashboarding",
    "weight": 9,
    "objective": "Implement Security Dashboarding to mature Digital Security Program Optimization and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Security Dashboarding defined, owned, and implemented consistently across the organization?\n2. Is Security Dashboarding measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Security Dashboarding.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Security Dashboarding artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Security Dashboarding leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate policy review and approval workflows",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.11",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Information Risk Management",
    "subDescription": "Manages risks to data, systems, people, and processes through structured practices",
    "practice": "Threat Modeling",
    "weight": 10,
    "objective": "Implement Threat Modeling to mature Information Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Threat Modeling defined, owned, and implemented consistently across the organization?\n2. Is Threat Modeling measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Threat Modeling.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. GRC platform (ServiceNow GRC, OneTrust, Archer)\n3. Policy mgmt/workflow (ServiceNow, Confluence)",
    "evidenceArtifacts": "1. Threat Modeling artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Threat Modeling leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.11.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Information Risk Management",
    "subDescription": "Manages risks to data, systems, people, and processes through structured practices",
    "practice": "Enterprise Risk Quantification",
    "weight": 9,
    "objective": "Implement Enterprise Risk Quantification to mature Information Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Enterprise Risk Quantification defined, owned, and implemented consistently across the organization?\n2. Is Enterprise Risk Quantification measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Enterprise Risk Quantification.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Enterprise Risk Quantification artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Enterprise Risk Quantification leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.11.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Information Risk Management",
    "subDescription": "Manages risks to data, systems, people, and processes through structured practices",
    "practice": "Residual Risk Indexing",
    "weight": 9,
    "objective": "Implement Residual Risk Indexing to mature Information Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Residual Risk Indexing defined, owned, and implemented consistently across the organization?\n2. Is Residual Risk Indexing measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Residual Risk Indexing.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Residual Risk Indexing artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Residual Risk Indexing leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.11.3",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Information Risk Management",
    "subDescription": "Manages risks to data, systems, people, and processes through structured practices",
    "practice": "Risk-tolerance Alignment",
    "weight": 9,
    "objective": "Implement Risk-tolerance Alignment to mature Information Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Risk-tolerance Alignment defined, owned, and implemented consistently across the organization?\n2. Is Risk-tolerance Alignment measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Risk-tolerance Alignment.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Risk-tolerance Alignment artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Risk-tolerance Alignment leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.12",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Third-Party Cyber Risk Management",
    "subDescription": "Governs cyber risks introduced by vendors and external partners",
    "practice": "Automated Vendor Scanning",
    "weight": 9,
    "objective": "Implement Automated Vendor Scanning to mature Third-Party Cyber Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Automated Vendor Scanning defined, owned, and implemented consistently across the organization?\n2. Is Automated Vendor Scanning measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated Vendor Scanning.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. Third-party risk automation (Whistic, OneTrust VRM)\n2. GRC platform (ServiceNow GRC, OneTrust, Archer)\n3. Policy mgmt/workflow (ServiceNow, Confluence)",
    "evidenceArtifacts": "1. Automated Vendor Scanning artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Automated Vendor Scanning leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.12.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Third-Party Cyber Risk Management",
    "subDescription": "Governs cyber risks introduced by vendors and external partners",
    "practice": "Tiered Assessments",
    "weight": 9,
    "objective": "Implement Tiered Assessments to mature Third-Party Cyber Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Tiered Assessments defined, owned, and implemented consistently across the organization?\n2. Is Tiered Assessments measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Tiered Assessments.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Tiered Assessments artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Tiered Assessments leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.12.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Third-Party Cyber Risk Management",
    "subDescription": "Governs cyber risks introduced by vendors and external partners",
    "practice": "Vendor Remediation SLA tracking",
    "weight": 9,
    "objective": "Implement Vendor Remediation SLA tracking to mature Third-Party Cyber Risk Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Vendor Remediation SLA tracking defined, owned, and implemented consistently across the organization?\n2. Is Vendor Remediation SLA tracking measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Vendor Remediation SLA tracking.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. Third-party risk automation (Whistic, OneTrust VRM)\n2. GRC platform (ServiceNow GRC, OneTrust, Archer)\n3. Policy mgmt/workflow (ServiceNow, Confluence)",
    "evidenceArtifacts": "1. Vendor Remediation SLA tracking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Vendor Remediation SLA tracking leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.13",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "User Awareness Training & Testing",
    "subDescription": "Builds a cyber-aware workforce using modern learning methods.",
    "practice": "Adaptive Phishing Tests",
    "weight": 8,
    "objective": "Implement Adaptive Phishing Tests to mature User Awareness Training & Testing and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Adaptive Phishing Tests defined, owned, and implemented consistently across the organization?\n2. Is Adaptive Phishing Tests measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Adaptive Phishing Tests.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Adaptive Phishing Tests artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Adaptive Phishing Tests leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.13.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "User Awareness Training & Testing",
    "subDescription": "Builds a cyber-aware workforce using modern learning methods.",
    "practice": "Microlearning Modules",
    "weight": 8,
    "objective": "Implement Microlearning Modules to mature User Awareness Training & Testing and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Microlearning Modules defined, owned, and implemented consistently across the organization?\n2. Is Microlearning Modules measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Microlearning Modules.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. GRC platform (ServiceNow GRC, OneTrust, Archer)\n3. Policy mgmt/workflow (ServiceNow, Confluence)",
    "evidenceArtifacts": "1. Microlearning Modules artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Microlearning Modules leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.13.2",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "User Awareness Training & Testing",
    "subDescription": "Builds a cyber-aware workforce using modern learning methods.",
    "practice": "Phish Susceptibility Average",
    "weight": 8,
    "objective": "Implement Phish Susceptibility Average to mature User Awareness Training & Testing and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Phish Susceptibility Average defined, owned, and implemented consistently across the organization?\n2. Is Phish Susceptibility Average measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Phish Susceptibility Average.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Phish Susceptibility Average artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Phish Susceptibility Average leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.14",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Continuous Compliance Management",
    "subDescription": "Ensures real-time, automated monitoring of compliance activities",
    "practice": "Policy-As-Code",
    "weight": 9,
    "objective": "Implement Policy-As-Code to mature Continuous Compliance Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Policy-As-Code defined, owned, and implemented consistently across the organization?\n2. Is Policy-As-Code measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Policy-As-Code.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Policy-As-Code artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Policy-As-Code leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DGA.14.1",
    "family": "DGA",
    "pillar": "Digital Governance & EA",
    "coreDimension": "Digital GRC Management",
    "coreDescription": "With digital, organizations need to reimagine how information security and compliance is governed and managed",
    "subDimension": "Continuous Compliance Management",
    "subDescription": "Ensures real-time, automated monitoring of compliance activities",
    "practice": "Automated Evidence Collection",
    "weight": 8,
    "objective": "Implement Automated Evidence Collection to mature Continuous Compliance Management and improve Digital GRC Management outcomes.",
    "assessmentQuestions": "1. Is Automated Evidence Collection defined, owned, and implemented consistently across the organization?\n2. Is Automated Evidence Collection measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated Evidence Collection.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ISO 27001:2022\n2. NIST CSF 2.0\n3. NIST SP 800-53 Rev.5\n4. SOC 2 Trust Services Criteria\n5. Secure Controls Framework (SCF)",
    "supportingTech": "1. GRC platform (ServiceNow GRC, OneTrust, Archer)\n2. Policy mgmt/workflow (ServiceNow, Confluence)\n3. Vendor risk tools (Whistic, SecurityScorecard)",
    "evidenceArtifacts": "1. Automated Evidence Collection artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "Risk Management, Policy Management, Vendor Risk, Audit & Assurance, Exception Management",
    "stakeholders": "CISO, Compliance Officer, Risk Manager, Security Architects, Internal Audit, Legal",
    "riskImplication": "Inadequate Automated Evidence Collection leads to audit gaps, inconsistent control execution, and higher regulatory/contract risk.",
    "transformationInitiative": "Digitize and automate GRC workflows and evidence collection",
    "industryOverlay": "1. Regulated (Financial Services, Healthcare, Public Sector)\n2. SaaS/PCI/SOC 2 environments"
  },
  {
    "id": "DXM.15",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Learning & Skills",
    "subDescription": "Builds a digitally fluent workforce capable of leveraging modern tooling",
    "practice": "Upskilling Pathways",
    "weight": 6,
    "objective": "Implement Upskilling Pathways to mature Digital Learning & Skills and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Upskilling Pathways defined, owned, and implemented consistently across the organization?\n2. Is Upskilling Pathways measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Upskilling Pathways.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Upskilling Pathways artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Upskilling Pathways increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.15.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Learning & Skills",
    "subDescription": "Builds a digitally fluent workforce capable of leveraging modern tooling",
    "practice": "AI-Assisted Learning",
    "weight": 6,
    "objective": "Implement AI-Assisted Learning to mature Digital Learning & Skills and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is AI-Assisted Learning defined, owned, and implemented consistently across the organization?\n2. Is AI-Assisted Learning measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for AI-Assisted Learning.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. MDM/UEM (Intune, Jamf)\n3. Collaboration (M365/Teams, Slack)",
    "evidenceArtifacts": "1. AI-Assisted Learning artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature AI-Assisted Learning increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.15.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Learning & Skills",
    "subDescription": "Builds a digitally fluent workforce capable of leveraging modern tooling",
    "practice": "Microlearning",
    "weight": 6,
    "objective": "Implement Microlearning to mature Digital Learning & Skills and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Microlearning defined, owned, and implemented consistently across the organization?\n2. Is Microlearning measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Microlearning.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. MDM/UEM (Intune, Jamf)\n3. Collaboration (M365/Teams, Slack)",
    "evidenceArtifacts": "1. Microlearning artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Microlearning increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.15.3",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Learning & Skills",
    "subDescription": "Builds a digitally fluent workforce capable of leveraging modern tooling",
    "practice": "Skill Benchmarking",
    "weight": 6,
    "objective": "Implement Skill Benchmarking to mature Digital Learning & Skills and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Skill Benchmarking defined, owned, and implemented consistently across the organization?\n2. Is Skill Benchmarking measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Skill Benchmarking.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Skill Benchmarking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Skill Benchmarking increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.15.4",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Learning & Skills",
    "subDescription": "Builds a digitally fluent workforce capable of leveraging modern tooling",
    "practice": "Proficiency Uplift",
    "weight": 6,
    "objective": "Implement Proficiency Uplift to mature Digital Learning & Skills and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Proficiency Uplift defined, owned, and implemented consistently across the organization?\n2. Is Proficiency Uplift measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Proficiency Uplift.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Proficiency Uplift artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Proficiency Uplift increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.16",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Workplace Enablement",
    "subDescription": "Enables seamless collaboration and productivity from any location or device",
    "practice": "Unified Communication Ecosystems",
    "weight": 6,
    "objective": "Implement Unified Communication Ecosystems to mature Digital Workplace Enablement and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Unified Communication Ecosystems defined, owned, and implemented consistently across the organization?\n2. Is Unified Communication Ecosystems measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Unified Communication Ecosystems.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Unified Communication Ecosystems artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Unified Communication Ecosystems increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.16.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Workplace Enablement",
    "subDescription": "Enables seamless collaboration and productivity from any location or device",
    "practice": "AI-enabled Productivity Tools",
    "weight": 6,
    "objective": "Implement AI-enabled Productivity Tools to mature Digital Workplace Enablement and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is AI-enabled Productivity Tools defined, owned, and implemented consistently across the organization?\n2. Is AI-enabled Productivity Tools measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for AI-enabled Productivity Tools.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. AI-enabled Productivity Tools artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature AI-enabled Productivity Tools increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.16.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Digital Workplace Enablement",
    "subDescription": "Enables seamless collaboration and productivity from any location or device",
    "practice": "Collaboration Adoption Rates",
    "weight": 6,
    "objective": "Implement Collaboration Adoption Rates to mature Digital Workplace Enablement and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Collaboration Adoption Rates defined, owned, and implemented consistently across the organization?\n2. Is Collaboration Adoption Rates measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Collaboration Adoption Rates.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Collaboration Adoption Rates artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Collaboration Adoption Rates increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.17",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Data Analytics & Reporting",
    "subDescription": "Uses BI/AI to produce timely insights that advance the business.",
    "practice": "BI Governance",
    "weight": 6,
    "objective": "Implement BI Governance to mature Data Analytics & Reporting and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is BI Governance defined, owned, and implemented consistently across the organization?\n2. Is BI Governance measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for BI Governance.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. BI Governance artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature BI Governance increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.17.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Data Analytics & Reporting",
    "subDescription": "Uses BI/AI to produce timely insights that advance the business.",
    "practice": "Data Visualization Standards",
    "weight": 6,
    "objective": "Implement Data Visualization Standards to mature Data Analytics & Reporting and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Data Visualization Standards defined, owned, and implemented consistently across the organization?\n2. Is Data Visualization Standards measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data Visualization Standards.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Data Visualization Standards artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Data Visualization Standards increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.18",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Data Analytics & Reporting",
    "subDescription": "Uses BI/AI to produce timely insights that advance the business.",
    "practice": "Automated Pipelines",
    "weight": 6,
    "objective": "Implement Automated Pipelines to mature Data Analytics & Reporting and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Automated Pipelines defined, owned, and implemented consistently across the organization?\n2. Is Automated Pipelines measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated Pipelines.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Automated Pipelines artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Automated Pipelines increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.19",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Collaboration & Productivity",
    "subDescription": "Enhances organizational output through integrated work platforms",
    "practice": "Digital Whiteboarding",
    "weight": 6,
    "objective": "Implement Digital Whiteboarding to mature Collaboration & Productivity and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Digital Whiteboarding defined, owned, and implemented consistently across the organization?\n2. Is Digital Whiteboarding measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Digital Whiteboarding.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Digital Whiteboarding artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Digital Whiteboarding increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.19.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Collaboration & Productivity",
    "subDescription": "Enhances organizational output through integrated work platforms",
    "practice": "Automated Workflows",
    "weight": 6,
    "objective": "Implement Automated Workflows to mature Collaboration & Productivity and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Automated Workflows defined, owned, and implemented consistently across the organization?\n2. Is Automated Workflows measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated Workflows.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Automated Workflows artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Automated Workflows increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.19.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Collaboration & Productivity",
    "subDescription": "Enhances organizational output through integrated work platforms",
    "practice": "Shared Repositories",
    "weight": 6,
    "objective": "Implement Shared Repositories to mature Collaboration & Productivity and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Shared Repositories defined, owned, and implemented consistently across the organization?\n2. Is Shared Repositories measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Shared Repositories.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Shared Repositories artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Shared Repositories increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.19.3",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Collaboration & Productivity",
    "subDescription": "Enhances organizational output through integrated work platforms",
    "practice": "Productivity KPIs",
    "weight": 6,
    "objective": "Implement Productivity KPIs to mature Collaboration & Productivity and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Productivity KPIs defined, owned, and implemented consistently across the organization?\n2. Is Productivity KPIs measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Productivity KPIs.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Productivity KPIs artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Productivity KPIs increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.19.4",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Collaboration & Productivity",
    "subDescription": "Enhances organizational output through integrated work platforms",
    "practice": "Process cycle-time reductions",
    "weight": 6,
    "objective": "Implement Process cycle-time reductions to mature Collaboration & Productivity and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Process cycle-time reductions defined, owned, and implemented consistently across the organization?\n2. Is Process cycle-time reductions measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Process cycle-time reductions.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Process cycle-time reductions artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Process cycle-time reductions increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.20",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Business Process Automation",
    "subDescription": "Automates repetitive work to increase efficiency and reduce human error.",
    "practice": "Robotic Process Automation (RPA)",
    "weight": 6,
    "objective": "Implement Robotic Process Automation (RPA) to mature Business Process Automation and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Robotic Process Automation (RPA) defined, owned, and implemented consistently across the organization?\n2. Is Robotic Process Automation (RPA) measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Robotic Process Automation (RPA).\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Robotic Process Automation (RPA) artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Robotic Process Automation (RPA) increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.20.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Business Process Automation",
    "subDescription": "Automates repetitive work to increase efficiency and reduce human error.",
    "practice": "Intelligent Automation",
    "weight": 6,
    "objective": "Implement Intelligent Automation to mature Business Process Automation and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Intelligent Automation defined, owned, and implemented consistently across the organization?\n2. Is Intelligent Automation measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Intelligent Automation.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Intelligent Automation artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Intelligent Automation increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.20.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Business Process Automation",
    "subDescription": "Automates repetitive work to increase efficiency and reduce human error.",
    "practice": "Process Mining",
    "weight": 6,
    "objective": "Implement Process Mining to mature Business Process Automation and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Process Mining defined, owned, and implemented consistently across the organization?\n2. Is Process Mining measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Process Mining.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Process Mining artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Process Mining increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.20.3",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Digital Workforce Optimization",
    "coreDescription": "A digitally dexterous workforce improves business outcomes",
    "subDimension": "Business Process Automation",
    "subDescription": "Automates repetitive work to increase efficiency and reduce human error.",
    "practice": "Automation ROI Tracking",
    "weight": 6,
    "objective": "Implement Automation ROI Tracking to mature Business Process Automation and improve Digital Workforce Optimization outcomes.",
    "assessmentQuestions": "1. Is Automation ROI Tracking defined, owned, and implemented consistently across the organization?\n2. Is Automation ROI Tracking measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automation ROI Tracking.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST NICE Framework\n2. ITIL 4\n3. ISO 27001:2022 (A.6/A.8)\n4. CIS Controls v8 (Basic Hygiene)",
    "supportingTech": "1. MDM/UEM (Intune, Jamf)\n2. Collaboration (M365/Teams, Slack)\n3. Security awareness (KnowBe4, Cofense)",
    "evidenceArtifacts": "1. Automation ROI Tracking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Endpoint compliance reports\n6. Awareness training completion",
    "relatedProcesses": "IT Operations, Endpoint Management, Identity & Access, Training & Awareness",
    "stakeholders": "CHRO, CIO, IT Service Desk, Collaboration/Endpoint Engineering, Security Awareness",
    "riskImplication": "Immature Automation ROI Tracking increases productivity loss, shadow IT, and endpoint/identity risk.",
    "transformationInitiative": "Digital workplace modernization and secure productivity enablement",
    "industryOverlay": "1. All Industries\n2. Elevated for distributed/remote workforce"
  },
  {
    "id": "DXM.21",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Modern Brand Look & Feel",
    "subDescription": "Ensures digital experiences are modern, appealing, and accessible",
    "practice": "Mobile-first UX",
    "weight": 6,
    "objective": "Implement Mobile-first UX to mature Modern Brand Look & Feel and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Mobile-first UX defined, owned, and implemented consistently across the organization?\n2. Is Mobile-first UX measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Mobile-first UX.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Mobile-first UX artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Mobile-first UX reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.21.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Modern Brand Look & Feel",
    "subDescription": "Ensures digital experiences are modern, appealing, and accessible",
    "practice": "Use of Branding Frameworks & Standards",
    "weight": 6,
    "objective": "Implement Use of Branding Frameworks & Standards to mature Modern Brand Look & Feel and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Use of Branding Frameworks & Standards defined, owned, and implemented consistently across the organization?\n2. Is Use of Branding Frameworks & Standards measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Use of Branding Frameworks & Standards.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Use of Branding Frameworks & Standards artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Use of Branding Frameworks & Standards reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.21.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Modern Brand Look & Feel",
    "subDescription": "Ensures digital experiences are modern, appealing, and accessible",
    "practice": "UX Satisfaction Metrics.",
    "weight": 6,
    "objective": "Implement UX Satisfaction Metrics. to mature Modern Brand Look & Feel and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is UX Satisfaction Metrics. defined, owned, and implemented consistently across the organization?\n2. Is UX Satisfaction Metrics. measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for UX Satisfaction Metrics..\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. UX Satisfaction Metrics. artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature UX Satisfaction Metrics. reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.22",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Customer Trust & Perspectives",
    "subDescription": "Strengthens trust through transparency, ethical data practices, and secure service.",
    "practice": "Trust Policies",
    "weight": 6,
    "objective": "Implement Trust Policies to mature Customer Trust & Perspectives and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Trust Policies defined, owned, and implemented consistently across the organization?\n2. Is Trust Policies measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Trust Policies.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Trust Policies artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Trust Policies reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.22.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Customer Trust & Perspectives",
    "subDescription": "Strengthens trust through transparency, ethical data practices, and secure service.",
    "practice": "Consent Management",
    "weight": 6,
    "objective": "Implement Consent Management to mature Customer Trust & Perspectives and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Consent Management defined, owned, and implemented consistently across the organization?\n2. Is Consent Management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Consent Management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Consent Management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Consent Management reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.22.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Customer Trust & Perspectives",
    "subDescription": "Strengthens trust through transparency, ethical data practices, and secure service.",
    "practice": "Customer Trust Index",
    "weight": 6,
    "objective": "Implement Customer Trust Index to mature Customer Trust & Perspectives and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Customer Trust Index defined, owned, and implemented consistently across the organization?\n2. Is Customer Trust Index measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Customer Trust Index.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Customer Trust Index artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Customer Trust Index reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.23",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Business Model Digital Optimization",
    "subDescription": "Digitally enhances revenue models, products, and experiences.",
    "practice": "Digital Marketplace Integration",
    "weight": 6,
    "objective": "Implement Digital Marketplace Integration to mature Business Model Digital Optimization and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Digital Marketplace Integration defined, owned, and implemented consistently across the organization?\n2. Is Digital Marketplace Integration measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Digital Marketplace Integration.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Digital Marketplace Integration artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Digital Marketplace Integration reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.23.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Business Model Digital Optimization",
    "subDescription": "Digitally enhances revenue models, products, and experiences.",
    "practice": "Pricing Optimization",
    "weight": 6,
    "objective": "Implement Pricing Optimization to mature Business Model Digital Optimization and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Pricing Optimization defined, owned, and implemented consistently across the organization?\n2. Is Pricing Optimization measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Pricing Optimization.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Pricing Optimization artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Pricing Optimization reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.23.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Business Model Digital Optimization",
    "subDescription": "Digitally enhances revenue models, products, and experiences.",
    "practice": "Digital Revenue",
    "weight": 6,
    "objective": "Implement Digital Revenue to mature Business Model Digital Optimization and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Digital Revenue defined, owned, and implemented consistently across the organization?\n2. Is Digital Revenue measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Digital Revenue.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Digital Revenue artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Digital Revenue reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.24",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Data-Driven Customer Experience",
    "subDescription": "Personalizes interactions using analytics and real-time behavior insights.",
    "practice": "Customer Journey Analytics",
    "weight": 6,
    "objective": "Implement Customer Journey Analytics to mature Data-Driven Customer Experience and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Customer Journey Analytics defined, owned, and implemented consistently across the organization?\n2. Is Customer Journey Analytics measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Customer Journey Analytics.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Customer Journey Analytics artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Customer Journey Analytics reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.24.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Data-Driven Customer Experience",
    "subDescription": "Personalizes interactions using analytics and real-time behavior insights.",
    "practice": "Personalization Engines",
    "weight": 6,
    "objective": "Implement Personalization Engines to mature Data-Driven Customer Experience and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Personalization Engines defined, owned, and implemented consistently across the organization?\n2. Is Personalization Engines measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Personalization Engines.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Personalization Engines artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Personalization Engines reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.24.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Data-Driven Customer Experience",
    "subDescription": "Personalizes interactions using analytics and real-time behavior insights.",
    "practice": "Customer Engagement Lift",
    "weight": 6,
    "objective": "Implement Customer Engagement Lift to mature Data-Driven Customer Experience and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Customer Engagement Lift defined, owned, and implemented consistently across the organization?\n2. Is Customer Engagement Lift measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Customer Engagement Lift.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Customer Engagement Lift artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Customer Engagement Lift reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.25",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Customer Appreciation & Loyalty",
    "subDescription": "Builds long-term loyalty through data-driven recognition strategies.",
    "practice": "Rewards Programs",
    "weight": 6,
    "objective": "Implement Rewards Programs to mature Customer Appreciation & Loyalty and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Rewards Programs defined, owned, and implemented consistently across the organization?\n2. Is Rewards Programs measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Rewards Programs.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Rewards Programs artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Rewards Programs reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.25.1",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Customer Appreciation & Loyalty",
    "subDescription": "Builds long-term loyalty through data-driven recognition strategies.",
    "practice": "Lifecycle Marketing",
    "weight": 6,
    "objective": "Implement Lifecycle Marketing to mature Customer Appreciation & Loyalty and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Lifecycle Marketing defined, owned, and implemented consistently across the organization?\n2. Is Lifecycle Marketing measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Lifecycle Marketing.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Lifecycle Marketing artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Lifecycle Marketing reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DXM.25.2",
    "family": "DXM",
    "pillar": "Digital Experience (DX)",
    "coreDimension": "Customer & Markets",
    "coreDescription": "The customer experience strategy is the cornerstone for your digital success",
    "subDimension": "Customer Appreciation & Loyalty",
    "subDescription": "Builds long-term loyalty through data-driven recognition strategies.",
    "practice": "Customer Retention Rate",
    "weight": 6,
    "objective": "Implement Customer Retention Rate to mature Customer Appreciation & Loyalty and improve Customer & Markets outcomes.",
    "assessmentQuestions": "1. Is Customer Retention Rate defined, owned, and implemented consistently across the organization?\n2. Is Customer Retention Rate measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Customer Retention Rate.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4 (Service Value System)\n2. ISO 9241-210 (Human-centred design)\n3. NIST Privacy Framework\n4. ISO 27018 (Cloud PII)",
    "supportingTech": "1. Product analytics (Amplitude, GA4)\n2. CRM/CS platforms (Salesforce, Gainsight)\n3. Consent/privacy mgmt (OneTrust, TrustArc)",
    "evidenceArtifacts": "1. Customer Retention Rate artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Journey maps/UX research\n6. Customer metrics (NPS/CSAT)",
    "relatedProcesses": "Product Management, Customer Success, Privacy, SDLC, Service Management",
    "stakeholders": "CMO, Product Management, Customer Success, UX, Privacy, Security",
    "riskImplication": "Immature Customer Retention Rate reduces customer trust, conversion, and increases privacy/brand risk.",
    "transformationInitiative": "Customer experience modernization and privacy-by-design program",
    "industryOverlay": "1. Consumer-facing and B2B SaaS\n2. Elevated where privacy regulations apply"
  },
  {
    "id": "DGM.01",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application Modernization",
    "subDescription": "Refactors legacy applications into cloud-native, scalable services",
    "practice": "Containerization",
    "weight": 8,
    "objective": "Implement Containerization to mature Application Modernization and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Containerization defined, owned, and implemented consistently across the organization?\n2. Is Containerization measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Containerization.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Containerization artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Containerization increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.01.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application Modernization",
    "subDescription": "Refactors legacy applications into cloud-native, scalable services",
    "practice": "Microservices",
    "weight": 8,
    "objective": "Implement Microservices to mature Application Modernization and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Microservices defined, owned, and implemented consistently across the organization?\n2. Is Microservices measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Microservices.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Microservices artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Microservices increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.01.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application Modernization",
    "subDescription": "Refactors legacy applications into cloud-native, scalable services",
    "practice": "API-first Design",
    "weight": 8,
    "objective": "Implement API-first Design to mature Application Modernization and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is API-first Design defined, owned, and implemented consistently across the organization?\n2. Is API-first Design measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for API-first Design.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. API-first Design artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature API-first Design increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.01.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application Modernization",
    "subDescription": "Refactors legacy applications into cloud-native, scalable services",
    "practice": "Deployment Frequency",
    "weight": 8,
    "objective": "Implement Deployment Frequency to mature Application Modernization and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Deployment Frequency defined, owned, and implemented consistently across the organization?\n2. Is Deployment Frequency measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Deployment Frequency.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Deployment Frequency artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Deployment Frequency increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.01.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application Modernization",
    "subDescription": "Refactors legacy applications into cloud-native, scalable services",
    "practice": "App Reliability",
    "weight": 8,
    "objective": "Implement App Reliability to mature Application Modernization and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is App Reliability defined, owned, and implemented consistently across the organization?\n2. Is App Reliability measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for App Reliability.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. App Reliability artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature App Reliability increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.02",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Modernized IT Infrastructure",
    "subDescription": "Builds a cloud-first, scalable, resilient infrastructure.",
    "practice": "Infrastructure-as-Code",
    "weight": 8,
    "objective": "Implement Infrastructure-as-Code to mature Modernized IT Infrastructure and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Infrastructure-as-Code defined, owned, and implemented consistently across the organization?\n2. Is Infrastructure-as-Code measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Infrastructure-as-Code.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Infrastructure-as-Code artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Infrastructure-as-Code increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.02.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Modernized IT Infrastructure",
    "subDescription": "Builds a cloud-first, scalable, resilient infrastructure.",
    "practice": "Software-Defined Everything",
    "weight": 8,
    "objective": "Implement Software-Defined Everything to mature Modernized IT Infrastructure and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Software-Defined Everything defined, owned, and implemented consistently across the organization?\n2. Is Software-Defined Everything measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Software-Defined Everything.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Software-Defined Everything artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Software-Defined Everything increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.03",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Cloud Applications & Platforms",
    "subDescription": "Leverages standardized cloud services to accelerate innovation.",
    "practice": "Multi-cloud governance",
    "weight": 8,
    "objective": "Implement Multi-cloud governance to mature Cloud Applications & Platforms and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Multi-cloud governance defined, owned, and implemented consistently across the organization?\n2. Is Multi-cloud governance measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Multi-cloud governance.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Multi-cloud governance artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Multi-cloud governance increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.03.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Cloud Applications & Platforms",
    "subDescription": "Leverages standardized cloud services to accelerate innovation.",
    "practice": "PaaS adoption",
    "weight": 8,
    "objective": "Implement PaaS adoption to mature Cloud Applications & Platforms and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is PaaS adoption defined, owned, and implemented consistently across the organization?\n2. Is PaaS adoption measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for PaaS adoption.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. PaaS adoption artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature PaaS adoption increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.03.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Cloud Applications & Platforms",
    "subDescription": "Leverages standardized cloud services to accelerate innovation.",
    "practice": "Platform Resiliency",
    "weight": 8,
    "objective": "Implement Platform Resiliency to mature Cloud Applications & Platforms and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Platform Resiliency defined, owned, and implemented consistently across the organization?\n2. Is Platform Resiliency measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Platform Resiliency.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Platform Resiliency artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Platform Resiliency increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.03.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Cloud Applications & Platforms",
    "subDescription": "Leverages standardized cloud services to accelerate innovation.",
    "practice": "Cloud Cost Efficiency",
    "weight": 8,
    "objective": "Implement Cloud Cost Efficiency to mature Cloud Applications & Platforms and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Cloud Cost Efficiency defined, owned, and implemented consistently across the organization?\n2. Is Cloud Cost Efficiency measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cloud Cost Efficiency.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Cloud Cost Efficiency artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Cloud Cost Efficiency increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.04",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Modernized End-user Computing",
    "subDescription": "Transforms the digital workplace with secure, seamless endpoint ecosystems",
    "practice": "User and Entity Management",
    "weight": 8,
    "objective": "Implement User and Entity Management to mature Modernized End-user Computing and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is User and Entity Management defined, owned, and implemented consistently across the organization?\n2. Is User and Entity Management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for User and Entity Management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. User and Entity Management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature User and Entity Management increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.04.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Modernized End-user Computing",
    "subDescription": "Transforms the digital workplace with secure, seamless endpoint ecosystems",
    "practice": "Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI)",
    "weight": 8,
    "objective": "Implement Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI) to mature Modernized End-user Computing and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI) defined, owned, and implemented consistently across the organization?\n2. Is Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI) measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI).\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI) artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Desktop-as-a-Service/Virtual Desktop Infrastrcuture (DaaS/VDI) increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.04.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Modernized End-user Computing",
    "subDescription": "Transforms the digital workplace with secure, seamless endpoint ecosystems",
    "practice": "Endpoint Telemetry",
    "weight": 8,
    "objective": "Implement Endpoint Telemetry to mature Modernized End-user Computing and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Endpoint Telemetry defined, owned, and implemented consistently across the organization?\n2. Is Endpoint Telemetry measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Endpoint Telemetry.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Endpoint Telemetry artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Endpoint Telemetry increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.05",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Integrated Service Management",
    "subDescription": "Combines ITSM, automation, and observability for proactive operations",
    "practice": "AI Operations (AIOps)",
    "weight": 8,
    "objective": "Implement AI Operations (AIOps) to mature Integrated Service Management and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is AI Operations (AIOps) defined, owned, and implemented consistently across the organization?\n2. Is AI Operations (AIOps) measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for AI Operations (AIOps).\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. AI Operations (AIOps) artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature AI Operations (AIOps) increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.05.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Integrated Service Management",
    "subDescription": "Combines ITSM, automation, and observability for proactive operations",
    "practice": "XLA",
    "weight": 8,
    "objective": "Implement XLA to mature Integrated Service Management and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is XLA defined, owned, and implemented consistently across the organization?\n2. Is XLA measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for XLA.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. XLA artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature XLA increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.05.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Integrated Service Management",
    "subDescription": "Combines ITSM, automation, and observability for proactive operations",
    "practice": "Automated incident Routing",
    "weight": 9,
    "objective": "Implement Automated incident Routing to mature Integrated Service Management and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Automated incident Routing defined, owned, and implemented consistently across the organization?\n2. Is Automated incident Routing measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated incident Routing.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Automated incident Routing artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Automated incident Routing increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.06",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application & Systems Interoperability",
    "subDescription": "Ensures seamless data exchange across platforms and ecosystems.",
    "practice": "API Lifecycle Management",
    "weight": 8,
    "objective": "Implement API Lifecycle Management to mature Application & Systems Interoperability and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is API Lifecycle Management defined, owned, and implemented consistently across the organization?\n2. Is API Lifecycle Management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for API Lifecycle Management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. API Lifecycle Management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature API Lifecycle Management increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.06.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application & Systems Interoperability",
    "subDescription": "Ensures seamless data exchange across platforms and ecosystems.",
    "practice": "Event-driven Architecture.",
    "weight": 8,
    "objective": "Implement Event-driven Architecture. to mature Application & Systems Interoperability and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Event-driven Architecture. defined, owned, and implemented consistently across the organization?\n2. Is Event-driven Architecture. measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Event-driven Architecture..\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. ITSM (ServiceNow, Jira Service Mgmt)\n3. Observability (Datadog, Grafana, Azure Monitor)",
    "evidenceArtifacts": "1. Event-driven Architecture. artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Event-driven Architecture. increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.06.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Infrastructure & Operations",
    "coreDescription": "A foundational digital infrastructure to maintain durable and reliable operations.",
    "subDimension": "Application & Systems Interoperability",
    "subDescription": "Ensures seamless data exchange across platforms and ecosystems.",
    "practice": "Integration Latency",
    "weight": 8,
    "objective": "Implement Integration Latency to mature Application & Systems Interoperability and improve Digital Infrastructure & Operations outcomes.",
    "assessmentQuestions": "1. Is Integration Latency defined, owned, and implemented consistently across the organization?\n2. Is Integration Latency measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Integration Latency.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. ITIL 4\n2. ISO/IEC 20000-1\n3. NIST SP 800-53 (CM/CP/SC)\n4. CIS Controls v8\n5. SRE practices (Google SRE)",
    "supportingTech": "1. ITSM (ServiceNow, Jira Service Mgmt)\n2. Observability (Datadog, Grafana, Azure Monitor)\n3. IaC/Config mgmt (Terraform, Ansible)",
    "evidenceArtifacts": "1. Integration Latency artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Change/release records\n6. Monitoring/availability reports",
    "relatedProcesses": "IT Service Management, Change/Release, Capacity & Availability, Disaster Recovery",
    "stakeholders": "CIO/CTO, Infrastructure & Cloud Ops, SRE/Platform Engineering, Network, ITSM",
    "riskImplication": "Immature Integration Latency increases outage risk, operational toil, and inability to recover from disruptions.",
    "transformationInitiative": "Platform engineering and ITSM modernization (cloud-first operations)",
    "industryOverlay": "1. All Industries\n2. Elevated for 24x7/mission-critical operations"
  },
  {
    "id": "DGM.07",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Governance",
    "subDescription": "Establishes enterprise-wide rules for data availability, integrity, security, and lifecycle.",
    "practice": "Data Stewardship",
    "weight": 8,
    "objective": "Implement Data Stewardship to mature Data Governance and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data Stewardship defined, owned, and implemented consistently across the organization?\n2. Is Data Stewardship measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "7\u00b7\u00b7",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Data Stewardship artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data Stewardship drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.07.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Governance",
    "subDescription": "Establishes enterprise-wide rules for data availability, integrity, security, and lifecycle.",
    "practice": "Master Data Management",
    "weight": 9,
    "objective": "Implement Master Data Management to mature Data Governance and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Master Data Management defined, owned, and implemented consistently across the organization?\n2. Is Master Data Management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Master Data Management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Master Data Management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Master Data Management drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.07.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Governance",
    "subDescription": "Establishes enterprise-wide rules for data availability, integrity, security, and lifecycle.",
    "practice": "Data Quality Management",
    "weight": 9,
    "objective": "Implement Data Quality Management to mature Data Governance and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data Quality Management defined, owned, and implemented consistently across the organization?\n2. Is Data Quality Management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data Quality Management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Data Quality Management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data Quality Management drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.07.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Governance",
    "subDescription": "Establishes enterprise-wide rules for data availability, integrity, security, and lifecycle.",
    "practice": "Data Policy Enforcement",
    "weight": 9,
    "objective": "Implement Data Policy Enforcement to mature Data Governance and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data Policy Enforcement defined, owned, and implemented consistently across the organization?\n2. Is Data Policy Enforcement measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data Policy Enforcement.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Data Policy Enforcement artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data Policy Enforcement drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.08",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Democratization",
    "subDescription": "Ensures all business functions can securely access, interpret, and use data to drive decisions without relying solely on IT specialists",
    "practice": "Data Literacy Programs",
    "weight": 2,
    "objective": "Enable secure self-service access to trusted data for all business functions so that decisions at every level are informed by timely, governed information rather than ad-hoc spreadsheets or gatekept reports.",
    "assessmentQuestions": "1. Do business users have governed, role-based self-service access to curated data sets and dashboards without relying on IT for every request?\n2. Has the organization implemented formal data literacy, training, and usage guidelines for non-technical staff?\n3. Are there clear policies defining which data can be democratized and which must remain restricted or masked?",
    "testingProcedures": "1. Review data access models, user groups, and permission sets in the BI and data catalog platforms.\n2. Sample training records, course materials, and completion reports for data-literacy programs.\n3. Inspect a sample of shared datasets and dashboards to confirm masking, anonymization, and RBAC alignment with classification policies.",
    "complianceFrameworks": "\u00b7  DAMA-DMBOK (Data Governance & Stewardship).\n\u00b7  ISO 8000 & ISO 25012 (Data quality).\n\u00b7  NIST CSF PR.DS (Data Security), ISO/IEC 27018 (Protection of PII in cloud).\n\u00b7  GDPR/CCPA principles for purpose limitation and data minimization where applicable",
    "supportingTech": "\u00b7  Data catalog and business glossary platforms (e.g., Azure Purview, Collibra).\n\u00b7  Self-service BI platforms (Power BI, Tableau, Looker).\n\u00b7  Data literacy and enablement programs; onboarding playbooks.\n\u00b7  RBAC/ABAC tied to HR source of truth and identity governance.",
    "evidenceArtifacts": "1. Data access matrices and role definitions.\n2. Catalog screenshots showing ownership, stewardship, and classifications.\n3. Sample curated datasets with masking rules.\n4. Training curricula, attendance/completion reports, and communications.\n5. Governance committee minutes documenting democratization decisions.",
    "relatedProcesses": "1. Data Governance and Stewardship.\n2. Identity & Access Management (IAM).\n3. Privacy & Data Protection program.\n4. Change Management for data models and semantic layers.\n5. Information Security Policy Management.",
    "stakeholders": "1. CDO / Head of Data & Analytics.\n2. CIO, CISO / Data Protection Officer.\n3. Business Unit Leaders & Product Owners.\n4. Data Stewards and BI Platform Owners.\n5. HR / Learning & Development for data-literacy content",
    "riskImplication": "If weak, decisions remain driven by intuition and shadow spreadsheets, increasing regulatory, data-leakage, and misreporting risk while slowing time-to-insight across the organization.",
    "transformationInitiative": "Implement an Enterprise Data Democratization Program combining governed self-service analytics, data catalog rollout, and formal data-literacy training.",
    "industryOverlay": "1. All Industries\n2. Heightened emphasis for Financial Services, Healthcare, Public Sector, and Critical Infrastructure due to regulatory expectations for transparent, well-governed data use."
  },
  {
    "id": "DGM.08.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Democratization",
    "subDescription": "Ensures all business functions can securely access, interpret, and use data to drive decisions without relying solely on IT specialists",
    "practice": "Federated data access governance",
    "weight": 8,
    "objective": "Implement Federated data access governance to mature Data Democratization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Federated data access governance defined, owned, and implemented consistently across the organization?\n2. Is Federated data access governance measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Federated data access governance.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Federated data access governance artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Federated data access governance drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Implement an Enterprise Data Democratization Program combining governed self-service analytics, data catalog rollout, and formal data-literacy training.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.08.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Democratization",
    "subDescription": "Ensures all business functions can securely access, interpret, and use data to drive decisions without relying solely on IT specialists",
    "practice": "Self-service analytics enablement",
    "weight": 8,
    "objective": "Implement Self-service analytics enablement to mature Data Democratization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Self-service analytics enablement defined, owned, and implemented consistently across the organization?\n2. Is Self-service analytics enablement measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Self-service analytics enablement.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Self-service analytics enablement artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Self-service analytics enablement drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Implement an Enterprise Data Democratization Program combining governed self-service analytics, data catalog rollout, and formal data-literacy training.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.08.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Democratization",
    "subDescription": "Ensures all business functions can securely access, interpret, and use data to drive decisions without relying solely on IT specialists",
    "practice": "Data cataloging platforms",
    "weight": 9,
    "objective": "Implement Data cataloging platforms to mature Data Democratization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data cataloging platforms defined, owned, and implemented consistently across the organization?\n2. Is Data cataloging platforms measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data cataloging platforms.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog (Purview, Collibra)\n2. Data catalog/governance (Collibra, Alation, Purview)\n3. ETL/ELT & pipelines (dbt, ADF)",
    "evidenceArtifacts": "1. Data cataloging platforms artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data cataloging platforms drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Implement an Enterprise Data Democratization Program combining governed self-service analytics, data catalog rollout, and formal data-literacy training.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.08.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data Democratization",
    "subDescription": "Ensures all business functions can securely access, interpret, and use data to drive decisions without relying solely on IT specialists",
    "practice": "Natural-language data querying",
    "weight": 8,
    "objective": "Implement Natural-language data querying to mature Data Democratization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Natural-language data querying defined, owned, and implemented consistently across the organization?\n2. Is Natural-language data querying measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Natural-language data querying.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Natural-language data querying artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Natural-language data querying drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Implement an Enterprise Data Democratization Program combining governed self-service analytics, data catalog rollout, and formal data-literacy training.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "Centralized BI governance",
    "weight": 2,
    "objective": "Build a scalable, governed analytics platform that consolidates reporting, advanced analytics, and AI/ML into a single ecosystem, enabling near real-time insight generation.",
    "assessmentQuestions": "1. Is there a standardized enterprise BI/analytics platform with defined governance, rather than multiple competing tools?\n2. Are semantic models, KPIs, and metrics centrally defined and reused across reports?\n3. Does the platform support advanced analytics and ML workloads with appropriate MLOps processes?",
    "testingProcedures": "1. Review the analytics platform inventory, governance charter, and exception register.\n2. Inspect semantic models, KPI catalogs, and version control practices.\n3. Assess MLOps pipeline definitions, model registry, and monitoring dashboards.\n4. Review performance/refresh monitoring and incident records",
    "complianceFrameworks": "1. DAMA-DMBOK (Analytics & Reporting).\n2. ISO 27001/27018 for securing analytics environments.\n3. NIST AI Risk Management Framework (where AI/ML is applied).\n4. Internal BI governance standards and data architecture patterns",
    "supportingTech": "1. Enterprise BI platform (Power BI Premium, Tableau Server, Qlik, etc.).\n2. Data warehouse / lakehouse platforms (Snowflake, Synapse, Databricks).\n3. MLOps tools (Azure ML, SageMaker, MLflow, DataRobot).\n4. BI Center of Excellence (CoE) with intake, standards, and support",
    "evidenceArtifacts": "1. BI governance charter and platform standards.\n2. Architecture diagrams of the analytics stack.\n3. KPI catalog, semantic model documentation.\n4. Model registry entries, deployment logs, and monitoring dashboards.\n5. SLA reports for BI refresh and performance.",
    "relatedProcesses": "1. Platform Engineering & Cloud Operations.\n2. Data Governance & Data Quality.\n3. IAM for analytics tools.\n4. DevSecOps & MLOps pipelines",
    "stakeholders": "1. CDO / Head of Analytics.\n2. CIO, CISO.\n3. BI Platform Owner, Data Engineers, Data Scientists.\n4. Business Analytics Leads and Product Owners",
    "riskImplication": "Fragmented analytics landscape, conflicting KPIs, and unsupported AI projects; difficult to prove \u201csingle source of truth\u201d; increased operational and strategic risk from poor decisions.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. Banking\n2. Insurance\n3. Retail\n4. Telecom\n5. Transportation\n6. SaaS Providers"
  },
  {
    "id": "DGM.09.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "MLOps and model lifecycle management",
    "weight": 8,
    "objective": "Implement MLOps and model lifecycle management to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is MLOps and model lifecycle management defined, owned, and implemented consistently across the organization?\n2. Is MLOps and model lifecycle management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for MLOps and model lifecycle management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. MLOps and model lifecycle management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor MLOps and model lifecycle management drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "Real-time streaming analytics",
    "weight": 8,
    "objective": "Implement Real-time streaming analytics to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Real-time streaming analytics defined, owned, and implemented consistently across the organization?\n2. Is Real-time streaming analytics measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Real-time streaming analytics.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Data catalog/governance (Collibra, Alation, Purview)\n3. ETL/ELT & pipelines (dbt, ADF)",
    "evidenceArtifacts": "1. Real-time streaming analytics artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Real-time streaming analytics drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "Data pipeline orchestration and version control",
    "weight": 8,
    "objective": "Implement Data pipeline orchestration and version control to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data pipeline orchestration and version control defined, owned, and implemented consistently across the organization?\n2. Is Data pipeline orchestration and version control measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data pipeline orchestration and version control.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Data pipeline orchestration and version control artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data pipeline orchestration and version control drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "Lakehouse and data mesh architectures",
    "weight": 8,
    "objective": "Implement Lakehouse and data mesh architectures to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Lakehouse and data mesh architectures defined, owned, and implemented consistently across the organization?\n2. Is Lakehouse and data mesh architectures measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Lakehouse and data mesh architectures.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Data catalog/governance (Collibra, Alation, Purview)\n3. ETL/ELT & pipelines (dbt, ADF)",
    "evidenceArtifacts": "1. Lakehouse and data mesh architectures artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Lakehouse and data mesh architectures drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "BI refresh SLAs, query performance SLAs",
    "weight": 8,
    "objective": "Implement BI refresh SLAs, query performance SLAs to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is BI refresh SLAs, query performance SLAs defined, owned, and implemented consistently across the organization?\n2. Is BI refresh SLAs, query performance SLAs measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for BI refresh SLAs, query performance SLAs.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. BI refresh SLAs, query performance SLAs artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor BI refresh SLAs, query performance SLAs drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09.6",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "Mobile-ready BI dashboards.",
    "weight": 8,
    "objective": "Implement Mobile-ready BI dashboards. to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Mobile-ready BI dashboards. defined, owned, and implemented consistently across the organization?\n2. Is Mobile-ready BI dashboards. measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Mobile-ready BI dashboards..\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Data catalog/governance (Collibra, Alation, Purview)\n3. ETL/ELT & pipelines (dbt, ADF)",
    "evidenceArtifacts": "1. Mobile-ready BI dashboards. artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Mobile-ready BI dashboards. drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.09.7",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "BI & Data Analytics Platform Enablement",
    "subDescription": "Establishes scalable, governed analytics platforms that enable descriptive, predictive, and prescriptive insights at enterprise scale.",
    "practice": "AI-driven insights",
    "weight": 8,
    "objective": "Implement AI-driven insights to mature BI & Data Analytics Platform Enablement and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is AI-driven insights defined, owned, and implemented consistently across the organization?\n2. Is AI-driven insights measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for AI-driven insights.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. AI-driven insights artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor AI-driven insights drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Unified Analytics Platform Program: standardize BI tools, build shared semantic models, implement MLOps, and retire legacy reporting stacks.",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "ETL/ELT modernization",
    "weight": 3,
    "objective": "Implement ETL/ELT modernization to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is ETL/ELT modernization defined, owned, and implemented consistently across the organization?\n2. Is ETL/ELT modernization measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for ETL/ELT modernization.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. ETL/ELT modernization artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor ETL/ELT modernization drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "API-first integration",
    "weight": 8,
    "objective": "Implement API-first integration to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is API-first integration defined, owned, and implemented consistently across the organization?\n2. Is API-first integration measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for API-first integration.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. API-first integration artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor API-first integration drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "Event-driven architecture",
    "weight": 8,
    "objective": "Implement Event-driven architecture to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Event-driven architecture defined, owned, and implemented consistently across the organization?\n2. Is Event-driven architecture measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Event-driven architecture.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Data catalog/governance (Collibra, Alation, Purview)\n3. ETL/ELT & pipelines (dbt, ADF)",
    "evidenceArtifacts": "1. Event-driven architecture artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Event-driven architecture drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "Master data management",
    "weight": 9,
    "objective": "Implement Master data management to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Master data management defined, owned, and implemented consistently across the organization?\n2. Is Master data management measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Master data management.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Master data management artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Master data management drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "Data lineage and impact analysis",
    "weight": 9,
    "objective": "Implement Data lineage and impact analysis to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data lineage and impact analysis defined, owned, and implemented consistently across the organization?\n2. Is Data lineage and impact analysis measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data lineage and impact analysis.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. Data catalog/governance (Collibra, Alation, Purview)\n3. ETL/ELT & pipelines (dbt, ADF)",
    "evidenceArtifacts": "1. Data lineage and impact analysis artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data lineage and impact analysis drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "Integration platforms",
    "weight": 8,
    "objective": "Implement Integration platforms to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Integration platforms defined, owned, and implemented consistently across the organization?\n2. Is Integration platforms measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Integration platforms.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Integration platforms artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Integration platforms drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.6",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "Cloud-based transformation patterns",
    "weight": 8,
    "objective": "Implement Cloud-based transformation patterns to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Cloud-based transformation patterns defined, owned, and implemented consistently across the organization?\n2. Is Cloud-based transformation patterns measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cloud-based transformation patterns.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Cloud-based transformation patterns artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Cloud-based transformation patterns drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.10.7",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Data integration & Transformation",
    "subDescription": "Unifies data from disparate systems and transforms it into reliable, analytics-ready formats to support enterprise modernization",
    "practice": "Data timeliness, accuracy, and completeness KPIs.",
    "weight": 8,
    "objective": "Implement Data timeliness, accuracy, and completeness KPIs. to mature Data integration & Transformation and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Data timeliness, accuracy, and completeness KPIs. defined, owned, and implemented consistently across the organization?\n2. Is Data timeliness, accuracy, and completeness KPIs. measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Data timeliness, accuracy, and completeness KPIs..\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Data timeliness, accuracy, and completeness KPIs. artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Data timeliness, accuracy, and completeness KPIs. drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.11",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Database Platform Optimization",
    "subDescription": "Modernizes data platforms for performance, scalability, resilience, and cost efficiency across cloud and hybrid environments.",
    "practice": "Performance tuning, indexing strategy, and workload management.",
    "weight": 1,
    "objective": "Implement Performance tuning, indexing strategy, and workload management. to mature Database Platform Optimization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Performance tuning, indexing strategy, and workload management. defined, owned, and implemented consistently across the organization?\n2. Is Performance tuning, indexing strategy, and workload management. measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Performance tuning, indexing strategy, and workload management..\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Performance tuning, indexing strategy, and workload management. artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Performance tuning, indexing strategy, and workload management. drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.11.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Database Platform Optimization",
    "subDescription": "Modernizes data platforms for performance, scalability, resilience, and cost efficiency across cloud and hybrid environments.",
    "practice": "Cloud migration of legacy databases",
    "weight": 8,
    "objective": "Implement Cloud migration of legacy databases to mature Database Platform Optimization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Cloud migration of legacy databases defined, owned, and implemented consistently across the organization?\n2. Is Cloud migration of legacy databases measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cloud migration of legacy databases.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Cloud migration of legacy databases artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Cloud migration of legacy databases drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.11.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Database Platform Optimization",
    "subDescription": "Modernizes data platforms for performance, scalability, resilience, and cost efficiency across cloud and hybrid environments.",
    "practice": "High-availability clustering and geo-redundancy",
    "weight": 8,
    "objective": "Implement High-availability clustering and geo-redundancy to mature Database Platform Optimization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is High-availability clustering and geo-redundancy defined, owned, and implemented consistently across the organization?\n2. Is High-availability clustering and geo-redundancy measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for High-availability clustering and geo-redundancy.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. High-availability clustering and geo-redundancy artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor High-availability clustering and geo-redundancy drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.11.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Database Platform Optimization",
    "subDescription": "Modernizes data platforms for performance, scalability, resilience, and cost efficiency across cloud and hybrid environments.",
    "practice": "Automated backup, recovery, archiving, and encryption",
    "weight": 10,
    "objective": "Implement Automated backup, recovery, archiving, and encryption to mature Database Platform Optimization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Automated backup, recovery, archiving, and encryption defined, owned, and implemented consistently across the organization?\n2. Is Automated backup, recovery, archiving, and encryption measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated backup, recovery, archiving, and encryption.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Automated backup, recovery, archiving, and encryption artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Automated backup, recovery, archiving, and encryption drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.11.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Database Platform Optimization",
    "subDescription": "Modernizes data platforms for performance, scalability, resilience, and cost efficiency across cloud and hybrid environments.",
    "practice": "Cost/performance benchmarking",
    "weight": 8,
    "objective": "Implement Cost/performance benchmarking to mature Database Platform Optimization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Cost/performance benchmarking defined, owned, and implemented consistently across the organization?\n2. Is Cost/performance benchmarking measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cost/performance benchmarking.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Cost/performance benchmarking artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Cost/performance benchmarking drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.11.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Data & Information Management",
    "coreDescription": "Data is the lifeblood of digital and requires capable management",
    "subDimension": "Database Platform Optimization",
    "subDescription": "Modernizes data platforms for performance, scalability, resilience, and cost efficiency across cloud and hybrid environments.",
    "practice": "Zero-trust database access",
    "weight": 8,
    "objective": "Implement Zero-trust database access to mature Database Platform Optimization and improve Data & Information Management outcomes.",
    "assessmentQuestions": "1. Is Zero-trust database access defined, owned, and implemented consistently across the organization?\n2. Is Zero-trust database access measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Zero-trust database access.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. DAMA-DMBOK2\n2. ISO 8000 (Data Quality)\n3. NIST Privacy Framework\n4. ISO 27001:2022\n5. NIST SP 800-53 (AU/SC/AC)",
    "supportingTech": "1. Data catalog/governance (Collibra, Alation, Purview)\n2. ETL/ELT & pipelines (dbt, ADF)\n3. Data quality/observability (Monte Carlo, Great Expectations)",
    "evidenceArtifacts": "1. Zero-trust database access artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Data catalog/lineage exports\n6. Data quality scorecards",
    "relatedProcesses": "Data Governance, Data Lifecycle Management, Privacy Management, SDLC/DataOps",
    "stakeholders": "CDO, Data Owners/Stewards, Data Architects, Analytics Lead, Privacy Officer, Security",
    "riskImplication": "Poor Zero-trust database access drives data quality issues, privacy exposure, and unreliable analytics/AI outcomes.",
    "transformationInitiative": "Enterprise data governance, catalog, and analytics modernization program",
    "industryOverlay": "1. All Industries\n2. Elevated where sensitive/regulated data is processed"
  },
  {
    "id": "DGM.12",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Security Architecture Optimization",
    "subDescription": "Establishes a modern zero-trust security architecture enabling secure business operations across hybrid and cloud ecosystems.",
    "practice": "Enterprise zero-trust",
    "weight": 2,
    "objective": "Implement Enterprise zero-trust to mature Security Architecture Optimization and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Enterprise zero-trust defined, owned, and implemented consistently across the organization?\n2. Is Enterprise zero-trust measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Enterprise zero-trust.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Enterprise zero-trust artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Enterprise zero-trust increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.12.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Security Architecture Optimization",
    "subDescription": "Establishes a modern zero-trust security architecture enabling secure business operations across hybrid and cloud ecosystems.",
    "practice": "Secure-by-design and secure-by-default",
    "weight": 9,
    "objective": "Implement Secure-by-design and secure-by-default to mature Security Architecture Optimization and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Secure-by-design and secure-by-default defined, owned, and implemented consistently across the organization?\n2. Is Secure-by-design and secure-by-default measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Secure-by-design and secure-by-default.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Secure-by-design and secure-by-default artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Secure-by-design and secure-by-default increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.12.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Security Architecture Optimization",
    "subDescription": "Establishes a modern zero-trust security architecture enabling secure business operations across hybrid and cloud ecosystems.",
    "practice": "Architecture review boards",
    "weight": 9,
    "objective": "Implement Architecture review boards to mature Security Architecture Optimization and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Architecture review boards defined, owned, and implemented consistently across the organization?\n2. Is Architecture review boards measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Architecture review boards.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n3. IAM/PAM (Entra ID/Okta, CyberArk)",
    "evidenceArtifacts": "1. Architecture review boards artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Architecture review boards increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.12.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Security Architecture Optimization",
    "subDescription": "Establishes a modern zero-trust security architecture enabling secure business operations across hybrid and cloud ecosystems.",
    "practice": "Secure multi-cloud landing zones",
    "weight": 9,
    "objective": "Implement Secure multi-cloud landing zones to mature Security Architecture Optimization and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Secure multi-cloud landing zones defined, owned, and implemented consistently across the organization?\n2. Is Secure multi-cloud landing zones measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Secure multi-cloud landing zones.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Secure multi-cloud landing zones artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Secure multi-cloud landing zones increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.12.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Security Architecture Optimization",
    "subDescription": "Establishes a modern zero-trust security architecture enabling secure business operations across hybrid and cloud ecosystems.",
    "practice": "Configuration Drift detection",
    "weight": 10,
    "objective": "Implement Configuration Drift detection to mature Security Architecture Optimization and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Configuration Drift detection defined, owned, and implemented consistently across the organization?\n2. Is Configuration Drift detection measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Configuration Drift detection.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Configuration Drift detection artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Configuration Drift detection increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.12.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Security Architecture Optimization",
    "subDescription": "Establishes a modern zero-trust security architecture enabling secure business operations across hybrid and cloud ecosystems.",
    "practice": "Architectural conformance scoring",
    "weight": 9,
    "objective": "Implement Architectural conformance scoring to mature Security Architecture Optimization and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Architectural conformance scoring defined, owned, and implemented consistently across the organization?\n2. Is Architectural conformance scoring measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Architectural conformance scoring.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Architectural conformance scoring artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Architectural conformance scoring increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "Unified SOC operations",
    "weight": 3,
    "objective": "Ensure consistent identity-based access control across systems and cloud environments.",
    "assessmentQuestions": "Does the organization apply consistent IAM policies across all platforms?",
    "testingProcedures": "1. Review IAM architecture diagrams \n2. Review access review procedures.",
    "complianceFrameworks": "1. NIST 800-63\n2. CIS Control 6\n3. ISO 27001: A.9",
    "supportingTech": "1. Azure AD\n2. Okta\n3. AWS IAM\n4. Conditional Access Policies",
    "evidenceArtifacts": "1. Unified SOC operations artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "1. Review Access control matrix\n2. Review Audit logs\n3. Review user provisioning workflows",
    "stakeholders": "1. Security Engineering\n2. IT Operations\n3. Application Owners",
    "riskImplication": "Access sprawl and privilege creep risks",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. Healthcare\n2. Financial Services"
  },
  {
    "id": "DGM.13.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "Threat intelligence automation",
    "weight": 10,
    "objective": "Implement Threat intelligence automation to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Threat intelligence automation defined, owned, and implemented consistently across the organization?\n2. Is Threat intelligence automation measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Threat intelligence automation.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n3. IAM/PAM (Entra ID/Okta, CyberArk)",
    "evidenceArtifacts": "1. Threat intelligence automation artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Threat intelligence automation increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "Continuous attack surface monitoring",
    "weight": 10,
    "objective": "Implement Continuous attack surface monitoring to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Continuous attack surface monitoring defined, owned, and implemented consistently across the organization?\n2. Is Continuous attack surface monitoring measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Continuous attack surface monitoring.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Continuous attack surface monitoring artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Continuous attack surface monitoring increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "Purple teaming",
    "weight": 10,
    "objective": "Implement Purple teaming to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Purple teaming defined, owned, and implemented consistently across the organization?\n2. Is Purple teaming measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Purple teaming.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n3. IAM/PAM (Entra ID/Okta, CyberArk)",
    "evidenceArtifacts": "1. Purple teaming artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Purple teaming increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13.3.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "Proactive adversary simulation",
    "weight": 10,
    "objective": "Implement Proactive adversary simulation to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Proactive adversary simulation defined, owned, and implemented consistently across the organization?\n2. Is Proactive adversary simulation measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Proactive adversary simulation.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Proactive adversary simulation artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Proactive adversary simulation increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "Cloud-native XDR",
    "weight": 10,
    "objective": "Implement Cloud-native XDR to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Cloud-native XDR defined, owned, and implemented consistently across the organization?\n2. Is Cloud-native XDR measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cloud-native XDR.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Cloud-native XDR artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Cloud-native XDR increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "SIEM modernization",
    "weight": 10,
    "objective": "Implement SIEM modernization to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is SIEM modernization defined, owned, and implemented consistently across the organization?\n2. Is SIEM modernization measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for SIEM modernization.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Sentinel, Splunk, XSOAR)\n2. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n3. IAM/PAM (Entra ID/Okta, CyberArk)",
    "evidenceArtifacts": "1. SIEM modernization artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak SIEM modernization increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.13.6",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Modernized Threat Management",
    "subDescription": "Detects and mitigates evolving threats using AI-driven analytics, automation, and proactive intelligence.",
    "practice": "AI-assisted triage",
    "weight": 10,
    "objective": "Implement AI-assisted triage to mature Modernized Threat Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is AI-assisted triage defined, owned, and implemented consistently across the organization?\n2. Is AI-assisted triage measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for AI-assisted triage.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. AI-assisted triage artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak AI-assisted triage increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Implement unified IAM architecture with automated role assignment",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Continuous authentication",
    "weight": 3,
    "objective": "Implement continuous monitoring and automated incident response for evolving threats.",
    "assessmentQuestions": "Are there real-time monitoring systems in place for internal and external threats?",
    "testingProcedures": "1. Check for use of SIEM and corresponding alert rules\n2. Review Incident Response playbooks and containment tools.",
    "complianceFrameworks": "1. MITRE ATT&CK\n2. NIST 800-61\n3. CIS Control 16",
    "supportingTech": "1. SIEM (e.g., Splunk, Sentinel)\n2. EDR\n3. SOAR tools",
    "evidenceArtifacts": "1. Continuous authentication artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "1. Review Security alerts\n2. Review playbooks\n3. Review incident logs\n4. Review RCA reports",
    "stakeholders": "1. SOC\n2. CISO\n3. Incident Response Teams",
    "riskImplication": "Undetected threats and delayed breach containment",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. Healthcare\n2. Tech\n3. Manufacturing"
  },
  {
    "id": "DGM.14.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Behavioral biometrics",
    "weight": 10,
    "objective": "Implement Behavioral biometrics to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Behavioral biometrics defined, owned, and implemented consistently across the organization?\n2. Is Behavioral biometrics measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Behavioral biometrics.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Behavioral biometrics artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Behavioral biometrics increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Identity proofing and verification workflows",
    "weight": 10,
    "objective": "Implement Identity proofing and verification workflows to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Identity proofing and verification workflows defined, owned, and implemented consistently across the organization?\n2. Is Identity proofing and verification workflows measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Identity proofing and verification workflows.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Identity proofing and verification workflows artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Identity proofing and verification workflows increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Fraud scoring models",
    "weight": 10,
    "objective": "Implement Fraud scoring models to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Fraud scoring models defined, owned, and implemented consistently across the organization?\n2. Is Fraud scoring models measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Fraud scoring models.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Fraud scoring models artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Fraud scoring models increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Credential theft detection",
    "weight": 10,
    "objective": "Implement Credential theft detection to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Credential theft detection defined, owned, and implemented consistently across the organization?\n2. Is Credential theft detection measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Credential theft detection.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Credential theft detection artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Credential theft detection increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Identity governance",
    "weight": 10,
    "objective": "Implement Identity governance to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Identity governance defined, owned, and implemented consistently across the organization?\n2. Is Identity governance measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Identity governance.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Identity governance artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Identity governance increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.6",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Biometric authentication (FIDO2, WebAuthN)",
    "weight": 10,
    "objective": "Implement Biometric authentication (FIDO2, WebAuthN) to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Biometric authentication (FIDO2, WebAuthN) defined, owned, and implemented consistently across the organization?\n2. Is Biometric authentication (FIDO2, WebAuthN) measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Biometric authentication (FIDO2, WebAuthN).\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Biometric authentication (FIDO2, WebAuthN) artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Biometric authentication (FIDO2, WebAuthN) increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.7",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Risk-based adaptive access",
    "weight": 10,
    "objective": "Implement Risk-based adaptive access to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Risk-based adaptive access defined, owned, and implemented consistently across the organization?\n2. Is Risk-based adaptive access measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Risk-based adaptive access.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Risk-based adaptive access artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Risk-based adaptive access increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.8",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Authentication success & failure KPIs",
    "weight": 10,
    "objective": "Implement Authentication success & failure KPIs to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Authentication success & failure KPIs defined, owned, and implemented consistently across the organization?\n2. Is Authentication success & failure KPIs measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Authentication success & failure KPIs.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Authentication success & failure KPIs artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Authentication success & failure KPIs increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.14.9",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Digital Identity Fraud Management",
    "subDescription": "Protects users and systems from credential theft, identity abuse, and account takeover risks.",
    "practice": "Passwordless Login",
    "weight": 10,
    "objective": "Implement Passwordless Login to mature Digital Identity Fraud Management and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Passwordless Login defined, owned, and implemented consistently across the organization?\n2. Is Passwordless Login measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Passwordless Login.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Passwordless Login artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Passwordless Login increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Deploy automated detection and response aligned to MITRE ATT&CK",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.15",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cybersecurity Assurance",
    "subDescription": "Validates that security controls, processes, technologies, and governance structures are functioning as intended.",
    "practice": "Continuous control testing",
    "weight": 3,
    "objective": "Implement Continuous control testing to mature Cybersecurity Assurance and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Continuous control testing defined, owned, and implemented consistently across the organization?\n2. Is Continuous control testing measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Continuous control testing.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Continuous control testing artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Continuous control testing increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.15.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cybersecurity Assurance",
    "subDescription": "Validates that security controls, processes, technologies, and governance structures are functioning as intended.",
    "practice": "Security Vulnerability Discovery",
    "weight": 9,
    "objective": "Vulnerability scanning, penetration testing, BAS tools",
    "assessmentQuestions": "1. Is Security Vulnerability Discovery defined, owned, and implemented consistently across the organization?\n2. Is Security Vulnerability Discovery measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Security Vulnerability Discovery.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Security Vulnerability Discovery artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Security Vulnerability Discovery increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.15.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cybersecurity Assurance",
    "subDescription": "Validates that security controls, processes, technologies, and governance structures are functioning as intended.",
    "practice": "Compliance-as-code",
    "weight": 9,
    "objective": "Implement Compliance-as-code to mature Cybersecurity Assurance and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Compliance-as-code defined, owned, and implemented consistently across the organization?\n2. Is Compliance-as-code measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Compliance-as-code.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Compliance-as-code artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Compliance-as-code increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.15.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cybersecurity Assurance",
    "subDescription": "Validates that security controls, processes, technologies, and governance structures are functioning as intended.",
    "practice": "Audit readiness automation",
    "weight": 10,
    "objective": "Implement Audit readiness automation to mature Cybersecurity Assurance and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Audit readiness automation defined, owned, and implemented consistently across the organization?\n2. Is Audit readiness automation measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Audit readiness automation.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. EA modeling (Archi, Sparx EA)\n2. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n3. IAM/PAM (Entra ID/Okta, CyberArk)",
    "evidenceArtifacts": "1. Audit readiness automation artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Audit readiness automation increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.15.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cybersecurity Assurance",
    "subDescription": "Validates that security controls, processes, technologies, and governance structures are functioning as intended.",
    "practice": "Remediation timelines",
    "weight": 9,
    "objective": "Implement Remediation timelines to mature Cybersecurity Assurance and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Remediation timelines defined, owned, and implemented consistently across the organization?\n2. Is Remediation timelines measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Remediation timelines.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Remediation timelines artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Remediation timelines increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.15.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cybersecurity Assurance",
    "subDescription": "Validates that security controls, processes, technologies, and governance structures are functioning as intended.",
    "practice": "Control effectiveness scoring",
    "weight": 9,
    "objective": "Implement Control effectiveness scoring to mature Cybersecurity Assurance and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Control effectiveness scoring defined, owned, and implemented consistently across the organization?\n2. Is Control effectiveness scoring measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Control effectiveness scoring.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Control effectiveness scoring artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Control effectiveness scoring increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "SOC playbooks",
    "weight": 3,
    "objective": "Implement SOC playbooks to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is SOC playbooks defined, owned, and implemented consistently across the organization?\n2. Is SOC playbooks measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for SOC playbooks.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Sentinel, Splunk, XSOAR)\n2. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n3. IAM/PAM (Entra ID/Okta, CyberArk)",
    "evidenceArtifacts": "1. SOC playbooks artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak SOC playbooks increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16.1",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "Tabletop Exercises (TTX)",
    "weight": 10,
    "objective": "Implement Tabletop Exercises (TTX) to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Tabletop Exercises (TTX) defined, owned, and implemented consistently across the organization?\n2. Is Tabletop Exercises (TTX) measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Tabletop Exercises (TTX).\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Tabletop Exercises (TTX) artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Tabletop Exercises (TTX) increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16.2",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "Automated Incident Response",
    "weight": 10,
    "objective": "Implement Automated Incident Response to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Automated Incident Response defined, owned, and implemented consistently across the organization?\n2. Is Automated Incident Response measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Automated Incident Response.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Automated Incident Response artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Automated Incident Response increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16.3",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "Business Continuity and Disaster Recovery",
    "weight": 10,
    "objective": "Implement Business Continuity and Disaster Recovery to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Business Continuity and Disaster Recovery defined, owned, and implemented consistently across the organization?\n2. Is Business Continuity and Disaster Recovery measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Business Continuity and Disaster Recovery.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Business Continuity and Disaster Recovery artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Business Continuity and Disaster Recovery increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16.4",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "Incident Response Orchestration",
    "weight": 10,
    "objective": "Implement Incident Response Orchestration to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Incident Response Orchestration defined, owned, and implemented consistently across the organization?\n2. Is Incident Response Orchestration measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Incident Response Orchestration.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Incident Response Orchestration artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Incident Response Orchestration increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16.5",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "Ransomware-proof storage",
    "weight": 10,
    "objective": "Implement Ransomware-proof storage to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Ransomware-proof storage defined, owned, and implemented consistently across the organization?\n2. Is Ransomware-proof storage measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Ransomware-proof storage.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Ransomware-proof storage artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Ransomware-proof storage increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  },
  {
    "id": "DGM.16.6",
    "family": "DGM",
    "pillar": "Digital Modernization",
    "coreDimension": "Digital Security Posture Management",
    "coreDescription": "With digital, cyber-exposure can create major impacts without proper management",
    "subDimension": "Cyber-incident Response & Recovery",
    "subDescription": "Ensures the organization can rapidly detect, contain, recover from, and learn from cybersecurity incidents.",
    "practice": "Cyber Range Simulation Environments",
    "weight": 10,
    "objective": "Implement Cyber Range Simulation Environments to mature Cyber-incident Response & Recovery and improve Digital Security Posture Management outcomes.",
    "assessmentQuestions": "1. Is Cyber Range Simulation Environments defined, owned, and implemented consistently across the organization?\n2. Is Cyber Range Simulation Environments measured (KPIs/KRIs) and continuously improved based on outcomes and risk?",
    "testingProcedures": "1. Review policies/standards and operating procedures for Cyber Range Simulation Environments.\n2. Interview process owners and validate roles/responsibilities (RACI).\n3. Sample evidence (tickets, logs, dashboards) from the last 90 days to confirm execution.",
    "complianceFrameworks": "1. NIST CSF 2.0\n2. ISO 27001:2022\n3. CIS Controls v8\n4. MITRE ATT&CK\n5. OWASP ASVS / Top 10",
    "supportingTech": "1. SIEM/SOAR (Splunk, Sentinel, Cortex XSOAR)\n2. IAM/PAM (Entra ID/Okta, CyberArk)\n3. CSPM/CIEM (Defender for Cloud, Wiz, Prisma)",
    "evidenceArtifacts": "1. Cyber Range Simulation Environments artifacts\n2. Process documentation / SOPs\n3. RACI / ownership artifacts\n4. KPIs/KRIs dashboards or reports\n5. Policies/standards and exception logs\n6. Audit logs / evidence packages",
    "relatedProcesses": "IAM, Vulnerability Management, Incident Response, Security Operations, DevSecOps",
    "stakeholders": "CISO, Security Engineering, IAM, SOC, DevSecOps, Risk & Compliance",
    "riskImplication": "Weak Cyber Range Simulation Environments increases likelihood of unauthorized access, lateral movement, or delayed detection/response.",
    "transformationInitiative": "Unified security architecture with automated prevention/detection/response",
    "industryOverlay": "1. All Industries\n2. Elevated for regulated/critical infrastructure"
  }
];
